{
  "cells": [
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:01.633263Z",
          "start_time": "2025-11-18T03:31:59.751419Z"
        },
        "id": "411bb6b89cd76bea"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import copy\n",
        "import pywt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "id": "411bb6b89cd76bea",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "786f8405b2601309"
      },
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "id": "786f8405b2601309"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:04.678962Z",
          "start_time": "2025-11-18T03:32:04.673661Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1922b74b6c1706d0",
        "outputId": "be9af701-5b0d-4b7c-c20c-8062fd5270d9"
      },
      "cell_type": "code",
      "source": [
        "target_var = \"onset\"   # 'add', 'onset', or 'hist'\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "\n",
        "BATCH_SIZE_ENC = 64       # CNN+BiLSTM encoder batch size\n",
        "EPOCHS_ENC = 40           # encoder training epochs\n",
        "LR_ENC = 1e-3             # encoder learning rate\n",
        "\n",
        "D_MODEL = 64              # CNN/BiLSTM feature dim\n",
        "GA_EMB_DIM = 16           # GA MLP embedding dim\n",
        "DROPOUT = 0.20\n",
        "\n",
        "WAVELET_NAME = \"db4\"\n",
        "WAVELET_LEVEL = 3         # raw + D1..D3 + A3 = 5 channels\n",
        "\n",
        "N_FOLDS = 5\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/ts2vec-05-yuntao\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "1922b74b6c1706d0",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x786a98f9f230>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:04.235351Z",
          "start_time": "2025-11-18T03:32:02.274215Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d93a90d61c31aabd",
        "outputId": "badae131-ceb7-4311-df0d-ff1312991a0c"
      },
      "cell_type": "code",
      "source": [
        "########## For Colab ##########\n",
        "!pip install ts2vec\n",
        "from ts2vec import TS2Vec\n",
        "\n",
        "########## Personal ##########\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open(f'/content/drive/MyDrive/datasets/dataset_{target_var}.json') as f:\n",
        "    content = f.read()\n",
        "    data = json.loads(content)\n",
        "\n",
        "########## Enterprise ##########\n",
        "# import gcsfs\n",
        "# fs = gcsfs.GCSFileSystem()\n",
        "# with fs.open('gs://modoo-eod/users/datasets/dataset_hist.json') as f:\n",
        "#     content = f.read()\n",
        "#     data = json.loads(content)\n",
        "\n",
        "######### Local ##########\n",
        "# with open(\"../datasets/dataset_onset.json\") as f:\n",
        "#     content=f.read()\n",
        "#     data=json.loads(content)"
      ],
      "id": "d93a90d61c31aabd",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ts2vec in /usr/local/lib/python3.12/dist-packages (0.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:04.636309Z",
          "start_time": "2025-11-18T03:32:04.239432Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d4d0637dd7abf28",
        "outputId": "c6727455-0e7d-4e99-eff9-bef4e7678244"
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_records(data)\n",
        "print(len(df), \"Measurements (raw)\")"
      ],
      "id": "6d4d0637dd7abf28",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3681 Measurements (raw)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2e925f95d5eb196e"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic Cleaning & Static Imputation"
      ],
      "id": "2e925f95d5eb196e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:04.814718Z",
          "start_time": "2025-11-18T03:32:04.751216Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a649055e50c4e8b1",
        "outputId": "79c3604d-962d-4cb3-f0bf-2316debbf592"
      },
      "cell_type": "code",
      "source": [
        "age_mean = np.mean([i[0] for i in df[\"static\"] if pd.notna(i[0])])\n",
        "bmi_mean = np.mean([i[1] for i in df[\"static\"] if pd.notna(i[1])])\n",
        "\n",
        "cleaned_data = []\n",
        "for _, m in enumerate(data):\n",
        "\n",
        "    # Remove measurements with empty windows\n",
        "    if len(m[\"uc_windows\"]) == 0 or len(m[\"fhr_windows\"]) == 0:\n",
        "        continue\n",
        "\n",
        "    # Handle NaN values in static\n",
        "    static = m[\"static\"].copy()\n",
        "    if pd.isna(static[0]):\n",
        "        static[0] = age_mean\n",
        "    if pd.isna(static[1]):\n",
        "        static[1] = bmi_mean\n",
        "\n",
        "    copy_m = m.copy()\n",
        "    copy_m[\"static\"] = static\n",
        "    cleaned_data.append(copy_m)\n",
        "\n",
        "cleaned_df = pd.DataFrame(cleaned_data)\n",
        "print(len(cleaned_df), \"Cleaned Measurements\")\n",
        "\n",
        "# 假定 static 最后一个元素是 gest_age_days\n",
        "cleaned_df[\"gest_age_weeks\"] = [(i[-1] // 7) + 1 for i in cleaned_df[\"static\"]]\n",
        "\n",
        "base_records = cleaned_df.to_dict(orient=\"records\")  # 原始不带任何 fold 修改的版本\n",
        "N = len(base_records)\n",
        "all_weeks = cleaned_df[\"gest_age_weeks\"].to_numpy()\n",
        "\n",
        "print(\"Gestational age weeks distribution:\")\n",
        "print(cleaned_df[\"gest_age_weeks\"].value_counts().sort_index())"
      ],
      "id": "a649055e50c4e8b1",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3661 Cleaned Measurements\n",
            "Gestational age weeks distribution:\n",
            "gest_age_weeks\n",
            "29     25\n",
            "30     72\n",
            "31    105\n",
            "32    157\n",
            "33    254\n",
            "34    313\n",
            "35    371\n",
            "36    443\n",
            "37    503\n",
            "38    544\n",
            "39    463\n",
            "40    321\n",
            "41     89\n",
            "42      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2596a51d53919e85"
      },
      "cell_type": "markdown",
      "source": [
        "### SWT Helper"
      ],
      "id": "2596a51d53919e85"
    },
    {
      "cell_type": "code",
      "id": "8d5ff935",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:04.884536Z",
          "start_time": "2025-11-18T03:32:04.882425Z"
        },
        "id": "8d5ff935"
      },
      "source": [
        "def sproc_wavelet_channels(x_1d: np.ndarray):\n",
        "    \"\"\"\n",
        "    SWT(db4, level=3) -> 通道: [raw, D1, D2, D3, A3]\n",
        "    \"\"\"\n",
        "    x_1d = np.asarray(x_1d, dtype=np.float32)\n",
        "    coeffs = pywt.swt(\n",
        "        x_1d, wavelet=WAVELET_NAME,\n",
        "        level=WAVELET_LEVEL,\n",
        "        trim_approx=False,\n",
        "        norm=True\n",
        "    )\n",
        "    # coeffs[l] = (cA_l, cD_l)\n",
        "    chans = [x_1d.astype(np.float32)]  # raw\n",
        "    for l in range(WAVELET_LEVEL):\n",
        "        cA, cD = coeffs[l]\n",
        "        chans.append(cD.astype(np.float32))   # D1..D3\n",
        "    chans.append(coeffs[-1][0].astype(np.float32))  # A3\n",
        "    X = np.stack(chans, axis=0)  # (C, T)\n",
        "\n",
        "    # 逐通道 robust 标准化\n",
        "    X = (X - np.median(X, axis=1, keepdims=True)) / (\n",
        "        np.std(X, axis=1, keepdims=True) + 1e-6\n",
        "    )\n",
        "    return X.astype(np.float32)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2b80d53bda529f02"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN+BiLSTM + GA encoder"
      ],
      "id": "2b80d53bda529f02"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:05.693480Z",
          "start_time": "2025-11-18T03:32:05.683670Z"
        },
        "id": "dac28eb1306dbfc7"
      },
      "cell_type": "code",
      "source": [
        "class SeqGADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for single signal (uc_raw or fhr_raw) + GA + target\n",
        "    在 __getitem__ 里做 SWT，输出 (C, T)。\n",
        "    \"\"\"\n",
        "    def __init__(self, measurements, signal_key):\n",
        "        self.measurements = measurements\n",
        "        self.signal_key = signal_key\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.measurements)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        m = self.measurements[idx]\n",
        "\n",
        "        seq = np.asarray(m[self.signal_key], dtype=np.float32)  # (T,)\n",
        "        x_ch = sproc_wavelet_channels(seq)                       # (C, T)\n",
        "\n",
        "        y = np.float32(m[\"target\"])\n",
        "        ga = np.float32(m[\"gest_age_weeks\"])  # gest age in weeks\n",
        "\n",
        "        return torch.from_numpy(x_ch), torch.tensor(y), torch.tensor(ga)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, d=1, p=0.0, groups_gn=8):\n",
        "        super().__init__()\n",
        "        pad = (k - 1) // 2 * d\n",
        "        self.conv = nn.Conv1d(\n",
        "            c_in, c_out, kernel_size=k, dilation=d,\n",
        "            padding=pad, bias=False\n",
        "        )\n",
        "        self.gn = nn.GroupNorm(num_groups=min(groups_gn, c_out),\n",
        "                               num_channels=c_out)\n",
        "        self.act = nn.SiLU()\n",
        "        self.dp = nn.Dropout(p)\n",
        "        self.res = (c_in == c_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.dp(self.act(self.gn(self.conv(x))))\n",
        "        return y + x if self.res else y\n",
        "\n",
        "\n",
        "class CNNBiLSTM_GA_Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    结构参考你原来的 CNNBiLSTM_GA_Reg：\n",
        "    - stem Conv1d\n",
        "    - 多层 ConvBlock (dilation k=9,7,5, d=1,2,4)\n",
        "    - BiLSTM\n",
        "    - GA MLP\n",
        "    - 输出:\n",
        "        pred: (B,)\n",
        "        feat: (B, D_MODEL + GA_EMB_DIM)\n",
        "    输入:\n",
        "        x:  (B, C, T)  SWT 通道\n",
        "        ga: (B,)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=5, d_model=64, ga_emb_dim=16, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.ga_emb_dim = ga_emb_dim\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, d_model, kernel_size=7, padding=3, bias=False),\n",
        "            nn.GroupNorm(num_groups=8, num_channels=d_model),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        self.cnn_blocks = nn.Sequential(\n",
        "            ConvBlock(d_model, d_model, k=9, d=1, p=dropout),\n",
        "            ConvBlock(d_model, d_model, k=7, d=2, p=dropout),\n",
        "            ConvBlock(d_model, d_model, k=5, d=4, p=dropout),\n",
        "            nn.Conv1d(d_model, d_model, kernel_size=1, bias=False),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model // 2,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        self.pre_pool_dp = nn.Dropout(dropout)\n",
        "\n",
        "        self.ga_mlp = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(16, ga_emb_dim),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        fuse_dim = d_model + ga_emb_dim\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(fuse_dim, fuse_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fuse_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, ga_scalar):\n",
        "        \"\"\"\n",
        "        x:  (B, C, T)\n",
        "        ga_scalar: (B,)\n",
        "        \"\"\"\n",
        "        f = self.stem(x)            # (B, d_model, T)\n",
        "        f = self.cnn_blocks(f)      # (B, d_model, T)\n",
        "\n",
        "        # BiLSTM\n",
        "        f_t = f.transpose(1, 2)     # (B, T, d_model)\n",
        "        y, _ = self.bilstm(f_t)     # (B, T, d_model)\n",
        "        y = self.pre_pool_dp(y)\n",
        "\n",
        "        # 时间维 average pooling\n",
        "        feat_seq = y.mean(dim=1)    # (B, d_model)\n",
        "\n",
        "        # GA embedding\n",
        "        ga_emb = self.ga_mlp(ga_scalar.unsqueeze(1))  # (B, ga_emb_dim)\n",
        "\n",
        "        fused = torch.cat([feat_seq, ga_emb], dim=1)  # (B, d_model + ga_emb_dim)\n",
        "        pred = self.head(fused).squeeze(-1)           # (B,)\n",
        "\n",
        "        return pred, fused\n",
        "\n",
        "\n",
        "def train_ga_encoder(measurements, signal_key):\n",
        "    \"\"\"\n",
        "    在 train 集上训练一个 SWT+CNN+BiLSTM+GA encoder（监督回归），返回训练好的模型。\n",
        "    \"\"\"\n",
        "    dataset = SeqGADataset(measurements, signal_key)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE_ENC,\n",
        "                        shuffle=True, drop_last=False)\n",
        "\n",
        "    model = CNNBiLSTM_GA_Encoder(\n",
        "        in_ch=5,\n",
        "        d_model=D_MODEL,\n",
        "        ga_emb_dim=GA_EMB_DIM,\n",
        "        dropout=DROPOUT,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR_ENC)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1, EPOCHS_ENC + 1):\n",
        "        epoch_loss = 0.0\n",
        "        n_samples = 0\n",
        "        for x_ch, y, ga in loader:\n",
        "            x_ch = x_ch.to(DEVICE)      # (B, C, T)\n",
        "            y = y.to(DEVICE)            # (B,)\n",
        "            ga = ga.to(DEVICE)          # (B,)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred, _ = model(x_ch, ga)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = x_ch.size(0)\n",
        "            epoch_loss += loss.item() * bs\n",
        "            n_samples += bs\n",
        "\n",
        "        print(f\"[{signal_key}] Epoch {epoch:03d} | Loss = {epoch_loss / n_samples:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def extract_ga_features(measurements, signal_key, model):\n",
        "    \"\"\"\n",
        "    用训练好的 encoder 抽取融合 GA 的特征。\n",
        "    返回 (N, d_model + ga_emb_dim) 的 numpy 数组。\n",
        "    \"\"\"\n",
        "    dataset = SeqGADataset(measurements, signal_key)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE_ENC,\n",
        "                        shuffle=False, drop_last=False)\n",
        "\n",
        "    model.eval()\n",
        "    feats = []\n",
        "    with torch.no_grad():\n",
        "        for x_ch, y, ga in loader:\n",
        "            x_ch = x_ch.to(DEVICE)       # (B, C, T)\n",
        "            ga = ga.to(DEVICE)\n",
        "            _, feat = model(x_ch, ga)    # (B, d_model + ga_emb_dim)\n",
        "            feats.append(feat.cpu().numpy())\n",
        "\n",
        "    feats = np.concatenate(feats, axis=0)\n",
        "    return feats"
      ],
      "id": "dac28eb1306dbfc7",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "edba72446464025"
      },
      "cell_type": "markdown",
      "source": [
        "### 5-Fold Cross-Validation"
      ],
      "id": "edba72446464025"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-18T03:32:19.954015Z",
          "start_time": "2025-11-18T03:32:08.840173Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b707ff1b9e93e53",
        "outputId": "ea71e130-d95a-4150-ede8-a93df9c04c07"
      },
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "all_true = []\n",
        "all_pred = []\n",
        "all_weeks_pred = []\n",
        "fold_metrics = []\n",
        "rows_week_fold = []\n",
        "all_pred_rows = []\n",
        "\n",
        "indices = np.arange(N)\n",
        "patients = list(set([i['mobile'] for i in base_records]))\n",
        "\n",
        "for fold, (tr_idx, te_idx) in enumerate(kf.split(np.arange(len(patients))), start=1):\n",
        "\n",
        "    print(f\"\\n===== Fold {fold}/{N_FOLDS} =====\")\n",
        "\n",
        "    # 每个 fold 用 base_records 的深拷贝，避免之前的修改污染\n",
        "\n",
        "    train_patients = [] ; test_patients = []\n",
        "    for i in tr_idx:\n",
        "        train_patients.append(patients[i])\n",
        "    for i in te_idx:\n",
        "        test_patients.append(patients[i])\n",
        "\n",
        "    train_recs = [] ; test_recs = []\n",
        "    for p in train_patients:\n",
        "        for m in base_records:\n",
        "            if m['mobile'] == p:\n",
        "                train_recs.append(copy.deepcopy(m))\n",
        "    for p in test_patients:\n",
        "        for m in base_records:\n",
        "            if m['mobile'] == p:\n",
        "                test_recs.append(copy.deepcopy(m))\n",
        "\n",
        "    print(f\"Train : {len(train_recs)} from {len(train_patients)} patients\")\n",
        "    print(f\"Test  : {len(test_recs)} from {len(test_patients)} patients\")\n",
        "\n",
        "    # train_recs = [copy.deepcopy(base_records[i]) for i in tr_idx]\n",
        "    # test_recs = [copy.deepcopy(base_records[i]) for i in te_idx]\n",
        "\n",
        "    # print(f\"Train samples: {len(train_recs)}, Test samples: {len(test_recs)}\")\n",
        "\n",
        "    # --- 1. 训练两个 encoder ---\n",
        "    print(\"\\n[Fold %d] Train encoder for uc_raw\" % fold)\n",
        "    encoder_uc = train_ga_encoder(train_recs, \"uc_raw\")\n",
        "\n",
        "    print(\"\\n[Fold %d] Train encoder for fhr_raw\" % fold)\n",
        "    encoder_fhr = train_ga_encoder(train_recs, \"fhr_raw\")\n",
        "\n",
        "    # --- 2. 提取 embedding ---\n",
        "    print(\"\\n[Fold %d] Extract embeddings (uc_raw & fhr_raw)\" % fold)\n",
        "    train_uc_embed = extract_ga_features(train_recs, \"uc_raw\", encoder_uc)\n",
        "    train_fhr_embed = extract_ga_features(train_recs, \"fhr_raw\", encoder_fhr)\n",
        "    test_uc_embed = extract_ga_features(test_recs, \"uc_raw\", encoder_uc)\n",
        "    test_fhr_embed = extract_ga_features(test_recs, \"fhr_raw\", encoder_fhr)\n",
        "\n",
        "    for i, e in enumerate(train_uc_embed):\n",
        "        train_recs[i][\"uc_raw\"] = e\n",
        "    for i, e in enumerate(train_fhr_embed):\n",
        "        train_recs[i][\"fhr_raw\"] = e\n",
        "    for i, e in enumerate(test_uc_embed):\n",
        "        test_recs[i][\"uc_raw\"] = e\n",
        "    for i, e in enumerate(test_fhr_embed):\n",
        "        test_recs[i][\"fhr_raw\"] = e\n",
        "\n",
        "    # --- 3. 聚合 window-level 特征 ---\n",
        "    for rec in train_recs:\n",
        "        uc_ws = rec[\"uc_windows\"]\n",
        "        fhr_ws = rec[\"fhr_windows\"]\n",
        "        # 原始是 list[dict]；这里才做 mean pooling\n",
        "        uc_w = np.array([[v for _, v in w.items()] for w in uc_ws],\n",
        "                        dtype=np.float32)\n",
        "        fhr_w = np.array([[v for _, v in w.items()] for w in fhr_ws],\n",
        "                         dtype=np.float32)\n",
        "        rec[\"uc_windows\"] = uc_w.mean(axis=0)\n",
        "        rec[\"fhr_windows\"] = fhr_w.mean(axis=0)\n",
        "\n",
        "    for rec in test_recs:\n",
        "        uc_ws = rec[\"uc_windows\"]\n",
        "        fhr_ws = rec[\"fhr_windows\"]\n",
        "        uc_w = np.array([[v for _, v in w.items()] for w in uc_ws],\n",
        "                        dtype=np.float32)\n",
        "        fhr_w = np.array([[v for _, v in w.items()] for w in fhr_ws],\n",
        "                         dtype=np.float32)\n",
        "        rec[\"uc_windows\"] = uc_w.mean(axis=0)\n",
        "        rec[\"fhr_windows\"] = fhr_w.mean(axis=0)\n",
        "\n",
        "    # 维度信息\n",
        "    FUSED_DIM = len(train_recs[0][\"uc_raw\"])        # 64 + 16 = 80\n",
        "    STATIC_DIM = len(train_recs[0][\"static\"])\n",
        "    UC_WIN_DIM = len(train_recs[0][\"uc_windows\"])   # 20\n",
        "    FHR_WIN_DIM = len(train_recs[0][\"fhr_windows\"]) # 24\n",
        "\n",
        "    FEATURE_ORDER = [\"uc_raw\", \"fhr_raw\", \"fhr_windows\", \"uc_windows\", \"static\"]\n",
        "    FEATURE_DIMS = {\n",
        "        \"uc_raw\": FUSED_DIM,\n",
        "        \"fhr_raw\": FUSED_DIM,\n",
        "        \"fhr_windows\": FHR_WIN_DIM,\n",
        "        \"uc_windows\": UC_WIN_DIM,\n",
        "        \"static\": STATIC_DIM,\n",
        "    }\n",
        "\n",
        "    def to_feature_vec(m):\n",
        "        parts = []\n",
        "        for k in FEATURE_ORDER:\n",
        "            v = np.asarray(m[k], dtype=np.float32).ravel()\n",
        "            assert v.size == FEATURE_DIMS[k], f\"{k} dim mismatch: {v.size} vs {FEATURE_DIMS[k]}\"\n",
        "            parts.append(v)\n",
        "        x = np.concatenate(parts).astype(np.float32, copy=False)\n",
        "        return x\n",
        "\n",
        "    def make_xy(measurements):\n",
        "        X = np.stack([to_feature_vec(m) for m in measurements], axis=0)\n",
        "        y = np.asarray([m[\"target\"] for m in measurements], dtype=np.float32).ravel()\n",
        "        weeks = np.asarray([m[\"gest_age_weeks\"] for m in measurements], dtype=np.int32)\n",
        "        return X, y, weeks\n",
        "\n",
        "    X_tr, y_tr, weeks_tr = make_xy(train_recs)\n",
        "    X_te, y_te, weeks_te = make_xy(test_recs)\n",
        "\n",
        "    print(f\"[Fold {fold}] X_tr={X_tr.shape}, X_te={X_te.shape}\")\n",
        "\n",
        "    # --- 4. 训练 LGBM ---\n",
        "    lgbm = LGBMRegressor(\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    lgbm.fit(X_tr, y_tr)\n",
        "\n",
        "    # --- 5. 预测 & 评估 ---\n",
        "    y_tr_pred = lgbm.predict(X_tr)\n",
        "    y_te_pred = lgbm.predict(X_te)\n",
        "\n",
        "    fold_train_mae = mean_absolute_error(y_tr, y_tr_pred)\n",
        "    fold_test_mae = mean_absolute_error(y_te, y_te_pred)\n",
        "    fold_test_rmse = np.sqrt(mean_squared_error(y_te, y_te_pred))\n",
        "    fold_test_r2 = r2_score(y_te, y_te_pred)\n",
        "\n",
        "    print(f\"[Fold {fold}] Train MAE = {fold_train_mae:.3f}\")\n",
        "    print(f\"[Fold {fold}] Test  MAE = {fold_test_mae:.3f}, RMSE = {fold_test_rmse:.3f}, R² = {fold_test_r2:.3f}\")\n",
        "\n",
        "    fold_metrics.append({\n",
        "        \"fold\": fold,\n",
        "        \"train_mae\": fold_train_mae,\n",
        "        \"test_mae\": fold_test_mae,\n",
        "        \"test_rmse\": fold_test_rmse,\n",
        "        \"test_r2\": fold_test_r2,\n",
        "    })\n",
        "\n",
        "    # 记录全体散点用\n",
        "    all_true.append(y_te)\n",
        "    all_pred.append(y_te_pred)\n",
        "    all_weeks_pred.append(weeks_te)\n",
        "\n",
        "    # <<< NEW: 保存当前 fold 的所有 test 样本的 actual / predicted\n",
        "    df_pred_fold = pd.DataFrame({\n",
        "        \"fold\": fold,\n",
        "        \"gest_age_weeks\": weeks_te.astype(int),\n",
        "        \"y_true\": y_te.astype(float),\n",
        "        \"y_pred\": y_te_pred.astype(float),\n",
        "    })\n",
        "    all_pred_rows.append(df_pred_fold)\n",
        "    # >>> END NEW\n",
        "\n",
        "    # --- 6. 每周 MAE ---\n",
        "    abs_err = np.abs(y_te_pred - y_te)\n",
        "    rows_this_fold = []\n",
        "    print(f\"[Fold {fold}] Test MAE by gestational week:\")\n",
        "    for w in sorted(np.unique(weeks_te)):\n",
        "        mask_w = (weeks_te == w)\n",
        "        n_w = int(mask_w.sum())\n",
        "        if n_w == 0:\n",
        "            continue\n",
        "        mae_w = float(abs_err[mask_w].mean())\n",
        "        rows_this_fold.append({\n",
        "            \"fold\": fold,\n",
        "            \"week\": int(w),\n",
        "            \"n_samples\": n_w,\n",
        "            \"mae\": mae_w,\n",
        "        })\n",
        "        print(f\"  Week {int(w):2d}: MAE={mae_w:.3f} (n={n_w})\")\n",
        "\n",
        "    # 加到总表\n",
        "    rows_week_fold.extend(rows_this_fold)\n",
        "\n",
        "    # 每折单独保存一个 per-week CSV（可选）\n",
        "    df_fold_week = pd.DataFrame(rows_this_fold)\n",
        "    fold_week_csv = os.path.join(OUT_DIR, f\"fold_{fold:02d}_week_mae.csv\")\n",
        "    df_fold_week.to_csv(fold_week_csv, index=False)\n",
        "    print(f\"[Saved] {fold_week_csv}\")\n",
        "\n",
        "# =======================\n",
        "# 汇总 prediction.csv\n",
        "# =======================\n",
        "\n",
        "df_pred_all = pd.concat(all_pred_rows, ignore_index=True)   # <<< NEW\n",
        "pred_csv_path = os.path.join(OUT_DIR, \"prediction.csv\")     # <<< NEW\n",
        "df_pred_all.to_csv(pred_csv_path, index=False)              # <<< NEW\n",
        "print(f\"[Saved] {pred_csv_path}\")                           # <<< NEW\n",
        "\n",
        "# =======================\n",
        "# 汇总结果\n",
        "# =======================\n",
        "\n",
        "all_true = np.concatenate(all_true)\n",
        "all_pred = np.concatenate(all_pred)\n",
        "all_weeks_pred = np.concatenate(all_weeks_pred)\n",
        "\n",
        "# 每折整体性能表\n",
        "df_folds = pd.DataFrame(fold_metrics)\n",
        "fold_summary_csv = os.path.join(OUT_DIR, \"fold_overall_metrics.csv\")\n",
        "df_folds.to_csv(fold_summary_csv, index=False)\n",
        "print(f\"[Saved] {fold_summary_csv}\")\n",
        "\n",
        "print(\"\\nOverall across folds:\")\n",
        "print(df_folds)\n",
        "\n",
        "# 每折 × 每周表\n",
        "df_week_fold = pd.DataFrame(rows_week_fold)\n",
        "week_fold_csv = os.path.join(OUT_DIR, \"week_mae_per_fold.csv\")\n",
        "df_week_fold.to_csv(week_fold_csv, index=False)\n",
        "print(f\"[Saved] {week_fold_csv}\")\n",
        "\n",
        "# 按 week 汇总 mean / std\n",
        "df_week_summary = df_week_fold.groupby(\"week\").agg(\n",
        "    mae_mean=(\"mae\", \"mean\"),\n",
        "    mae_std=(\"mae\", \"std\"),\n",
        "    n_total=(\"n_samples\", \"sum\"),\n",
        "    n_folds=(\"mae\", \"count\"),\n",
        ").reset_index()\n",
        "\n",
        "week_summary_csv = os.path.join(OUT_DIR, \"week_mae_summary.csv\")\n",
        "df_week_summary.to_csv(week_summary_csv, index=False)\n",
        "print(f\"[Saved] {week_summary_csv}\")\n",
        "\n",
        "print(\"\\nWeek-wise MAE summary:\")\n",
        "print(df_week_summary)\n",
        "\n",
        "# =======================\n",
        "# 可视化 1：bar chart with SD (per week MAE)\n",
        "# =======================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "x = df_week_summary[\"week\"].values\n",
        "y = df_week_summary[\"mae_mean\"].values\n",
        "yerr = df_week_summary[\"mae_std\"].values\n",
        "\n",
        "plt.bar(x, y, yerr=yerr, capsize=3)\n",
        "plt.xlabel(\"Gestational Age (weeks)\")\n",
        "plt.ylabel(\"MAE (days-to-onset)\")\n",
        "plt.title(\"Per-week MAE across 5-fold CV (mean ± SD)\")\n",
        "plt.xticks(x)\n",
        "plt.tight_layout()\n",
        "bar_path = os.path.join(OUT_DIR, \"week_mae_bar_with_sd.png\")\n",
        "plt.savefig(bar_path, dpi=200)\n",
        "plt.close()\n",
        "print(f\"[Saved] {bar_path}\")\n",
        "\n",
        "# =======================\n",
        "# 可视化 2：拟合散点图 (true vs pred)\n",
        "# =======================\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(all_true, all_pred, s=8, alpha=0.5)\n",
        "mn = float(min(all_true.min(), all_pred.min()))\n",
        "mx = float(max(all_true.max(), all_pred.max()))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", label=\"Ideal\")\n",
        "\n",
        "overall_mae = mean_absolute_error(all_true, all_pred)\n",
        "overall_rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
        "overall_r2 = r2_score(all_true, all_pred)\n",
        "\n",
        "plt.xlabel(\"True days to onset\")\n",
        "plt.ylabel(\"Predicted days to onset\")\n",
        "plt.title(f\"5-fold CV — True vs Pred\\nMAE={overall_mae:.2f}, RMSE={overall_rmse:.2f}, R²={overall_r2:.3f}\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "scatter_path = os.path.join(OUT_DIR, \"cv_true_vs_pred_scatter.png\")\n",
        "plt.savefig(scatter_path, dpi=200)\n",
        "plt.close()\n",
        "print(f\"[Saved] {scatter_path}\")"
      ],
      "id": "8b707ff1b9e93e53",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 1/5 =====\n",
            "Train : 2892 from 45 patients\n",
            "Test  : 769 from 12 patients\n",
            "\n",
            "[Fold 1] Train encoder for uc_raw\n",
            "[uc_raw] Epoch 001 | Loss = 697.7561\n",
            "[uc_raw] Epoch 002 | Loss = 411.7242\n",
            "[uc_raw] Epoch 003 | Loss = 389.0933\n",
            "[uc_raw] Epoch 004 | Loss = 370.7957\n",
            "[uc_raw] Epoch 005 | Loss = 351.3145\n",
            "[uc_raw] Epoch 006 | Loss = 332.0038\n",
            "[uc_raw] Epoch 007 | Loss = 325.1339\n",
            "[uc_raw] Epoch 008 | Loss = 315.4523\n",
            "[uc_raw] Epoch 009 | Loss = 309.9812\n",
            "[uc_raw] Epoch 010 | Loss = 300.9370\n",
            "[uc_raw] Epoch 011 | Loss = 284.9339\n",
            "[uc_raw] Epoch 012 | Loss = 265.7664\n",
            "[uc_raw] Epoch 013 | Loss = 252.2780\n",
            "[uc_raw] Epoch 014 | Loss = 230.2932\n",
            "[uc_raw] Epoch 015 | Loss = 221.4088\n",
            "[uc_raw] Epoch 016 | Loss = 185.8327\n",
            "[uc_raw] Epoch 017 | Loss = 181.3918\n",
            "[uc_raw] Epoch 018 | Loss = 156.7488\n",
            "[uc_raw] Epoch 019 | Loss = 137.2236\n",
            "[uc_raw] Epoch 020 | Loss = 136.1366\n",
            "[uc_raw] Epoch 021 | Loss = 127.8324\n",
            "[uc_raw] Epoch 022 | Loss = 111.5219\n",
            "[uc_raw] Epoch 023 | Loss = 100.7470\n",
            "[uc_raw] Epoch 024 | Loss = 93.0439\n",
            "[uc_raw] Epoch 025 | Loss = 94.8083\n",
            "[uc_raw] Epoch 026 | Loss = 93.4383\n",
            "[uc_raw] Epoch 027 | Loss = 87.6631\n",
            "[uc_raw] Epoch 028 | Loss = 84.6912\n",
            "[uc_raw] Epoch 029 | Loss = 81.5738\n",
            "[uc_raw] Epoch 030 | Loss = 82.1511\n",
            "[uc_raw] Epoch 031 | Loss = 81.8054\n",
            "[uc_raw] Epoch 032 | Loss = 77.3071\n",
            "[uc_raw] Epoch 033 | Loss = 79.6846\n",
            "[uc_raw] Epoch 034 | Loss = 80.3762\n",
            "[uc_raw] Epoch 035 | Loss = 80.3281\n",
            "[uc_raw] Epoch 036 | Loss = 78.9506\n",
            "[uc_raw] Epoch 037 | Loss = 71.9521\n",
            "[uc_raw] Epoch 038 | Loss = 76.1831\n",
            "[uc_raw] Epoch 039 | Loss = 75.5533\n",
            "[uc_raw] Epoch 040 | Loss = 73.2364\n",
            "\n",
            "[Fold 1] Train encoder for fhr_raw\n",
            "[fhr_raw] Epoch 001 | Loss = 648.9699\n",
            "[fhr_raw] Epoch 002 | Loss = 429.5771\n",
            "[fhr_raw] Epoch 003 | Loss = 409.3444\n",
            "[fhr_raw] Epoch 004 | Loss = 394.4530\n",
            "[fhr_raw] Epoch 005 | Loss = 389.0365\n",
            "[fhr_raw] Epoch 006 | Loss = 379.6042\n",
            "[fhr_raw] Epoch 007 | Loss = 369.3815\n",
            "[fhr_raw] Epoch 008 | Loss = 356.8222\n",
            "[fhr_raw] Epoch 009 | Loss = 349.2828\n",
            "[fhr_raw] Epoch 010 | Loss = 336.0511\n",
            "[fhr_raw] Epoch 011 | Loss = 316.4407\n",
            "[fhr_raw] Epoch 012 | Loss = 298.6036\n",
            "[fhr_raw] Epoch 013 | Loss = 286.9697\n",
            "[fhr_raw] Epoch 014 | Loss = 270.0074\n",
            "[fhr_raw] Epoch 015 | Loss = 258.3932\n",
            "[fhr_raw] Epoch 016 | Loss = 238.4469\n",
            "[fhr_raw] Epoch 017 | Loss = 220.0191\n",
            "[fhr_raw] Epoch 018 | Loss = 206.6722\n",
            "[fhr_raw] Epoch 019 | Loss = 186.1888\n",
            "[fhr_raw] Epoch 020 | Loss = 177.3644\n",
            "[fhr_raw] Epoch 021 | Loss = 156.4713\n",
            "[fhr_raw] Epoch 022 | Loss = 157.0775\n",
            "[fhr_raw] Epoch 023 | Loss = 140.1716\n",
            "[fhr_raw] Epoch 024 | Loss = 134.4422\n",
            "[fhr_raw] Epoch 025 | Loss = 119.9325\n",
            "[fhr_raw] Epoch 026 | Loss = 118.1351\n",
            "[fhr_raw] Epoch 027 | Loss = 124.1463\n",
            "[fhr_raw] Epoch 028 | Loss = 115.3175\n",
            "[fhr_raw] Epoch 029 | Loss = 118.6434\n",
            "[fhr_raw] Epoch 030 | Loss = 105.4767\n",
            "[fhr_raw] Epoch 031 | Loss = 109.7022\n",
            "[fhr_raw] Epoch 032 | Loss = 100.5960\n",
            "[fhr_raw] Epoch 033 | Loss = 100.3139\n",
            "[fhr_raw] Epoch 034 | Loss = 102.7174\n",
            "[fhr_raw] Epoch 035 | Loss = 97.6407\n",
            "[fhr_raw] Epoch 036 | Loss = 96.1056\n",
            "[fhr_raw] Epoch 037 | Loss = 99.7106\n",
            "[fhr_raw] Epoch 038 | Loss = 94.9808\n",
            "[fhr_raw] Epoch 039 | Loss = 101.2917\n",
            "[fhr_raw] Epoch 040 | Loss = 90.8377\n",
            "\n",
            "[Fold 1] Extract embeddings (uc_raw & fhr_raw)\n",
            "[Fold 1] X_tr=(2892, 212), X_te=(769, 212)\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007350 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 40894\n",
            "[LightGBM] [Info] Number of data points in the train set: 2892, number of used features: 198\n",
            "[LightGBM] [Info] Start training from score 26.230950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Train MAE = 0.043\n",
            "[Fold 1] Test  MAE = 5.340, RMSE = 7.090, R² = 0.826\n",
            "[Fold 1] Test MAE by gestational week:\n",
            "  Week 31: MAE=3.933 (n=8)\n",
            "  Week 32: MAE=5.223 (n=29)\n",
            "  Week 33: MAE=4.329 (n=61)\n",
            "  Week 34: MAE=5.964 (n=69)\n",
            "  Week 35: MAE=5.544 (n=89)\n",
            "  Week 36: MAE=7.405 (n=112)\n",
            "  Week 37: MAE=6.477 (n=113)\n",
            "  Week 38: MAE=4.744 (n=100)\n",
            "  Week 39: MAE=4.937 (n=107)\n",
            "  Week 40: MAE=2.600 (n=67)\n",
            "  Week 41: MAE=1.170 (n=14)\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/fold_01_week_mae.csv\n",
            "\n",
            "===== Fold 2/5 =====\n",
            "Train : 2903 from 45 patients\n",
            "Test  : 758 from 12 patients\n",
            "\n",
            "[Fold 2] Train encoder for uc_raw\n",
            "[uc_raw] Epoch 001 | Loss = 696.6121\n",
            "[uc_raw] Epoch 002 | Loss = 376.3167\n",
            "[uc_raw] Epoch 003 | Loss = 353.2174\n",
            "[uc_raw] Epoch 004 | Loss = 329.9570\n",
            "[uc_raw] Epoch 005 | Loss = 317.0782\n",
            "[uc_raw] Epoch 006 | Loss = 310.3385\n",
            "[uc_raw] Epoch 007 | Loss = 300.9177\n",
            "[uc_raw] Epoch 008 | Loss = 294.3403\n",
            "[uc_raw] Epoch 009 | Loss = 291.5218\n",
            "[uc_raw] Epoch 010 | Loss = 282.3551\n",
            "[uc_raw] Epoch 011 | Loss = 276.1286\n",
            "[uc_raw] Epoch 012 | Loss = 267.2071\n",
            "[uc_raw] Epoch 013 | Loss = 251.5687\n",
            "[uc_raw] Epoch 014 | Loss = 239.3580\n",
            "[uc_raw] Epoch 015 | Loss = 227.7518\n",
            "[uc_raw] Epoch 016 | Loss = 211.2764\n",
            "[uc_raw] Epoch 017 | Loss = 195.9499\n",
            "[uc_raw] Epoch 018 | Loss = 173.9855\n",
            "[uc_raw] Epoch 019 | Loss = 154.9678\n",
            "[uc_raw] Epoch 020 | Loss = 155.8081\n",
            "[uc_raw] Epoch 021 | Loss = 133.5444\n",
            "[uc_raw] Epoch 022 | Loss = 125.3309\n",
            "[uc_raw] Epoch 023 | Loss = 115.8882\n",
            "[uc_raw] Epoch 024 | Loss = 110.3506\n",
            "[uc_raw] Epoch 025 | Loss = 105.4718\n",
            "[uc_raw] Epoch 026 | Loss = 107.0213\n",
            "[uc_raw] Epoch 027 | Loss = 101.0052\n",
            "[uc_raw] Epoch 028 | Loss = 95.6325\n",
            "[uc_raw] Epoch 029 | Loss = 93.5970\n",
            "[uc_raw] Epoch 030 | Loss = 93.5288\n",
            "[uc_raw] Epoch 031 | Loss = 88.2552\n",
            "[uc_raw] Epoch 032 | Loss = 86.6892\n",
            "[uc_raw] Epoch 033 | Loss = 84.7771\n",
            "[uc_raw] Epoch 034 | Loss = 85.1724\n",
            "[uc_raw] Epoch 035 | Loss = 81.8578\n",
            "[uc_raw] Epoch 036 | Loss = 78.6717\n",
            "[uc_raw] Epoch 037 | Loss = 84.6963\n",
            "[uc_raw] Epoch 038 | Loss = 83.9891\n",
            "[uc_raw] Epoch 039 | Loss = 83.8288\n",
            "[uc_raw] Epoch 040 | Loss = 80.6449\n",
            "\n",
            "[Fold 2] Train encoder for fhr_raw\n",
            "[fhr_raw] Epoch 001 | Loss = 689.6836\n",
            "[fhr_raw] Epoch 002 | Loss = 377.7373\n",
            "[fhr_raw] Epoch 003 | Loss = 363.4535\n",
            "[fhr_raw] Epoch 004 | Loss = 358.8956\n",
            "[fhr_raw] Epoch 005 | Loss = 351.2045\n",
            "[fhr_raw] Epoch 006 | Loss = 342.0829\n",
            "[fhr_raw] Epoch 007 | Loss = 332.9191\n",
            "[fhr_raw] Epoch 008 | Loss = 327.5261\n",
            "[fhr_raw] Epoch 009 | Loss = 323.5474\n",
            "[fhr_raw] Epoch 010 | Loss = 314.7864\n",
            "[fhr_raw] Epoch 011 | Loss = 309.7755\n",
            "[fhr_raw] Epoch 012 | Loss = 300.4894\n",
            "[fhr_raw] Epoch 013 | Loss = 286.9275\n",
            "[fhr_raw] Epoch 014 | Loss = 266.9050\n",
            "[fhr_raw] Epoch 015 | Loss = 246.2864\n",
            "[fhr_raw] Epoch 016 | Loss = 230.4452\n",
            "[fhr_raw] Epoch 017 | Loss = 201.1903\n",
            "[fhr_raw] Epoch 018 | Loss = 173.2929\n",
            "[fhr_raw] Epoch 019 | Loss = 156.8853\n",
            "[fhr_raw] Epoch 020 | Loss = 137.0711\n",
            "[fhr_raw] Epoch 021 | Loss = 127.2053\n",
            "[fhr_raw] Epoch 022 | Loss = 114.5842\n",
            "[fhr_raw] Epoch 023 | Loss = 112.5175\n",
            "[fhr_raw] Epoch 024 | Loss = 107.1511\n",
            "[fhr_raw] Epoch 025 | Loss = 101.2809\n",
            "[fhr_raw] Epoch 026 | Loss = 96.9118\n",
            "[fhr_raw] Epoch 027 | Loss = 99.8818\n",
            "[fhr_raw] Epoch 028 | Loss = 98.1958\n",
            "[fhr_raw] Epoch 029 | Loss = 95.8117\n",
            "[fhr_raw] Epoch 030 | Loss = 98.2708\n",
            "[fhr_raw] Epoch 031 | Loss = 92.6814\n",
            "[fhr_raw] Epoch 032 | Loss = 93.7877\n",
            "[fhr_raw] Epoch 033 | Loss = 90.8553\n",
            "[fhr_raw] Epoch 034 | Loss = 97.2214\n",
            "[fhr_raw] Epoch 035 | Loss = 89.8247\n",
            "[fhr_raw] Epoch 036 | Loss = 90.6485\n",
            "[fhr_raw] Epoch 037 | Loss = 89.6183\n",
            "[fhr_raw] Epoch 038 | Loss = 87.2232\n",
            "[fhr_raw] Epoch 039 | Loss = 89.4818\n",
            "[fhr_raw] Epoch 040 | Loss = 84.9000\n",
            "\n",
            "[Fold 2] Extract embeddings (uc_raw & fhr_raw)\n",
            "[Fold 2] X_tr=(2903, 212), X_te=(758, 212)\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005379 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 40911\n",
            "[LightGBM] [Info] Number of data points in the train set: 2903, number of used features: 199\n",
            "[LightGBM] [Info] Start training from score 25.660493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Train MAE = 0.043\n",
            "[Fold 2] Test  MAE = 4.449, RMSE = 5.370, R² = 0.931\n",
            "[Fold 2] Test MAE by gestational week:\n",
            "  Week 29: MAE=3.221 (n=7)\n",
            "  Week 30: MAE=5.638 (n=29)\n",
            "  Week 31: MAE=6.669 (n=35)\n",
            "  Week 32: MAE=5.518 (n=35)\n",
            "  Week 33: MAE=6.264 (n=43)\n",
            "  Week 34: MAE=6.022 (n=58)\n",
            "  Week 35: MAE=5.370 (n=63)\n",
            "  Week 36: MAE=4.741 (n=87)\n",
            "  Week 37: MAE=3.989 (n=95)\n",
            "  Week 38: MAE=3.063 (n=118)\n",
            "  Week 39: MAE=3.881 (n=112)\n",
            "  Week 40: MAE=2.971 (n=71)\n",
            "  Week 41: MAE=0.939 (n=5)\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/fold_02_week_mae.csv\n",
            "\n",
            "===== Fold 3/5 =====\n",
            "Train : 2847 from 46 patients\n",
            "Test  : 814 from 11 patients\n",
            "\n",
            "[Fold 3] Train encoder for uc_raw\n",
            "[uc_raw] Epoch 001 | Loss = 618.4390\n",
            "[uc_raw] Epoch 002 | Loss = 380.8885\n",
            "[uc_raw] Epoch 003 | Loss = 360.8768\n",
            "[uc_raw] Epoch 004 | Loss = 348.0297\n",
            "[uc_raw] Epoch 005 | Loss = 330.2261\n",
            "[uc_raw] Epoch 006 | Loss = 318.1002\n",
            "[uc_raw] Epoch 007 | Loss = 310.4290\n",
            "[uc_raw] Epoch 008 | Loss = 306.0653\n",
            "[uc_raw] Epoch 009 | Loss = 296.5803\n",
            "[uc_raw] Epoch 010 | Loss = 291.1912\n",
            "[uc_raw] Epoch 011 | Loss = 286.6022\n",
            "[uc_raw] Epoch 012 | Loss = 278.1305\n",
            "[uc_raw] Epoch 013 | Loss = 273.7133\n",
            "[uc_raw] Epoch 014 | Loss = 260.1644\n",
            "[uc_raw] Epoch 015 | Loss = 250.4320\n",
            "[uc_raw] Epoch 016 | Loss = 233.3441\n",
            "[uc_raw] Epoch 017 | Loss = 224.9614\n",
            "[uc_raw] Epoch 018 | Loss = 206.3994\n",
            "[uc_raw] Epoch 019 | Loss = 190.3849\n",
            "[uc_raw] Epoch 020 | Loss = 172.4423\n",
            "[uc_raw] Epoch 021 | Loss = 155.4243\n",
            "[uc_raw] Epoch 022 | Loss = 137.8965\n",
            "[uc_raw] Epoch 023 | Loss = 125.5668\n",
            "[uc_raw] Epoch 024 | Loss = 119.2416\n",
            "[uc_raw] Epoch 025 | Loss = 113.5233\n",
            "[uc_raw] Epoch 026 | Loss = 102.9869\n",
            "[uc_raw] Epoch 027 | Loss = 99.6332\n",
            "[uc_raw] Epoch 028 | Loss = 95.9866\n",
            "[uc_raw] Epoch 029 | Loss = 102.5199\n",
            "[uc_raw] Epoch 030 | Loss = 96.0532\n",
            "[uc_raw] Epoch 031 | Loss = 89.5300\n",
            "[uc_raw] Epoch 032 | Loss = 89.2010\n",
            "[uc_raw] Epoch 033 | Loss = 91.1002\n",
            "[uc_raw] Epoch 034 | Loss = 89.5289\n",
            "[uc_raw] Epoch 035 | Loss = 86.9068\n",
            "[uc_raw] Epoch 036 | Loss = 87.5085\n",
            "[uc_raw] Epoch 037 | Loss = 87.6222\n",
            "[uc_raw] Epoch 038 | Loss = 86.5204\n",
            "[uc_raw] Epoch 039 | Loss = 88.1051\n",
            "[uc_raw] Epoch 040 | Loss = 86.4378\n",
            "\n",
            "[Fold 3] Train encoder for fhr_raw\n",
            "[fhr_raw] Epoch 001 | Loss = 663.5013\n",
            "[fhr_raw] Epoch 002 | Loss = 384.1270\n",
            "[fhr_raw] Epoch 003 | Loss = 366.1257\n",
            "[fhr_raw] Epoch 004 | Loss = 359.0616\n",
            "[fhr_raw] Epoch 005 | Loss = 346.0283\n",
            "[fhr_raw] Epoch 006 | Loss = 335.2248\n",
            "[fhr_raw] Epoch 007 | Loss = 319.1021\n",
            "[fhr_raw] Epoch 008 | Loss = 307.1760\n",
            "[fhr_raw] Epoch 009 | Loss = 299.3435\n",
            "[fhr_raw] Epoch 010 | Loss = 290.3947\n",
            "[fhr_raw] Epoch 011 | Loss = 269.4096\n",
            "[fhr_raw] Epoch 012 | Loss = 266.7289\n",
            "[fhr_raw] Epoch 013 | Loss = 245.1951\n",
            "[fhr_raw] Epoch 014 | Loss = 231.7176\n",
            "[fhr_raw] Epoch 015 | Loss = 209.1397\n",
            "[fhr_raw] Epoch 016 | Loss = 195.5890\n",
            "[fhr_raw] Epoch 017 | Loss = 176.2952\n",
            "[fhr_raw] Epoch 018 | Loss = 164.9324\n",
            "[fhr_raw] Epoch 019 | Loss = 147.5307\n",
            "[fhr_raw] Epoch 020 | Loss = 132.5907\n",
            "[fhr_raw] Epoch 021 | Loss = 123.6861\n",
            "[fhr_raw] Epoch 022 | Loss = 122.8923\n",
            "[fhr_raw] Epoch 023 | Loss = 113.8822\n",
            "[fhr_raw] Epoch 024 | Loss = 107.8576\n",
            "[fhr_raw] Epoch 025 | Loss = 99.4958\n",
            "[fhr_raw] Epoch 026 | Loss = 102.5927\n",
            "[fhr_raw] Epoch 027 | Loss = 103.5055\n",
            "[fhr_raw] Epoch 028 | Loss = 92.4909\n",
            "[fhr_raw] Epoch 029 | Loss = 94.9449\n",
            "[fhr_raw] Epoch 030 | Loss = 90.4842\n",
            "[fhr_raw] Epoch 031 | Loss = 88.0438\n",
            "[fhr_raw] Epoch 032 | Loss = 88.5827\n",
            "[fhr_raw] Epoch 033 | Loss = 91.3060\n",
            "[fhr_raw] Epoch 034 | Loss = 86.1928\n",
            "[fhr_raw] Epoch 035 | Loss = 85.8329\n",
            "[fhr_raw] Epoch 036 | Loss = 85.9635\n",
            "[fhr_raw] Epoch 037 | Loss = 93.1043\n",
            "[fhr_raw] Epoch 038 | Loss = 86.3990\n",
            "[fhr_raw] Epoch 039 | Loss = 87.2041\n",
            "[fhr_raw] Epoch 040 | Loss = 86.9835\n",
            "\n",
            "[Fold 3] Extract embeddings (uc_raw & fhr_raw)\n",
            "[Fold 3] X_tr=(2847, 212), X_te=(814, 212)\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007088 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 40892\n",
            "[LightGBM] [Info] Number of data points in the train set: 2847, number of used features: 199\n",
            "[LightGBM] [Info] Start training from score 25.573284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Train MAE = 0.052\n",
            "[Fold 3] Test  MAE = 5.172, RMSE = 6.329, R² = 0.898\n",
            "[Fold 3] Test MAE by gestational week:\n",
            "  Week 29: MAE=13.848 (n=14)\n",
            "  Week 30: MAE=7.243 (n=11)\n",
            "  Week 31: MAE=2.202 (n=15)\n",
            "  Week 32: MAE=1.987 (n=21)\n",
            "  Week 33: MAE=6.100 (n=60)\n",
            "  Week 34: MAE=6.683 (n=69)\n",
            "  Week 35: MAE=5.956 (n=87)\n",
            "  Week 36: MAE=6.043 (n=112)\n",
            "  Week 37: MAE=5.784 (n=122)\n",
            "  Week 38: MAE=4.893 (n=117)\n",
            "  Week 39: MAE=3.935 (n=109)\n",
            "  Week 40: MAE=1.873 (n=65)\n",
            "  Week 41: MAE=0.915 (n=12)\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/fold_03_week_mae.csv\n",
            "\n",
            "===== Fold 4/5 =====\n",
            "Train : 3072 from 46 patients\n",
            "Test  : 589 from 11 patients\n",
            "\n",
            "[Fold 4] Train encoder for uc_raw\n",
            "[uc_raw] Epoch 001 | Loss = 679.6620\n",
            "[uc_raw] Epoch 002 | Loss = 403.1255\n",
            "[uc_raw] Epoch 003 | Loss = 378.5281\n",
            "[uc_raw] Epoch 004 | Loss = 350.0578\n",
            "[uc_raw] Epoch 005 | Loss = 330.8941\n",
            "[uc_raw] Epoch 006 | Loss = 319.4835\n",
            "[uc_raw] Epoch 007 | Loss = 301.1830\n",
            "[uc_raw] Epoch 008 | Loss = 286.9255\n",
            "[uc_raw] Epoch 009 | Loss = 271.1158\n",
            "[uc_raw] Epoch 010 | Loss = 260.7544\n",
            "[uc_raw] Epoch 011 | Loss = 239.8143\n",
            "[uc_raw] Epoch 012 | Loss = 233.0143\n",
            "[uc_raw] Epoch 013 | Loss = 220.6428\n",
            "[uc_raw] Epoch 014 | Loss = 205.6386\n",
            "[uc_raw] Epoch 015 | Loss = 200.5583\n",
            "[uc_raw] Epoch 016 | Loss = 173.7380\n",
            "[uc_raw] Epoch 017 | Loss = 158.8984\n",
            "[uc_raw] Epoch 018 | Loss = 144.2058\n",
            "[uc_raw] Epoch 019 | Loss = 133.5326\n",
            "[uc_raw] Epoch 020 | Loss = 127.6856\n",
            "[uc_raw] Epoch 021 | Loss = 119.2265\n",
            "[uc_raw] Epoch 022 | Loss = 111.3611\n",
            "[uc_raw] Epoch 023 | Loss = 109.8296\n",
            "[uc_raw] Epoch 024 | Loss = 105.6597\n",
            "[uc_raw] Epoch 025 | Loss = 97.4033\n",
            "[uc_raw] Epoch 026 | Loss = 91.9427\n",
            "[uc_raw] Epoch 027 | Loss = 92.7545\n",
            "[uc_raw] Epoch 028 | Loss = 92.4066\n",
            "[uc_raw] Epoch 029 | Loss = 88.8113\n",
            "[uc_raw] Epoch 030 | Loss = 92.0705\n",
            "[uc_raw] Epoch 031 | Loss = 82.9048\n",
            "[uc_raw] Epoch 032 | Loss = 80.8473\n",
            "[uc_raw] Epoch 033 | Loss = 84.8611\n",
            "[uc_raw] Epoch 034 | Loss = 81.1549\n",
            "[uc_raw] Epoch 035 | Loss = 76.2934\n",
            "[uc_raw] Epoch 036 | Loss = 79.7700\n",
            "[uc_raw] Epoch 037 | Loss = 77.4212\n",
            "[uc_raw] Epoch 038 | Loss = 77.6657\n",
            "[uc_raw] Epoch 039 | Loss = 79.4287\n",
            "[uc_raw] Epoch 040 | Loss = 76.2261\n",
            "\n",
            "[Fold 4] Train encoder for fhr_raw\n",
            "[fhr_raw] Epoch 001 | Loss = 639.7669\n",
            "[fhr_raw] Epoch 002 | Loss = 403.0488\n",
            "[fhr_raw] Epoch 003 | Loss = 385.9570\n",
            "[fhr_raw] Epoch 004 | Loss = 372.0299\n",
            "[fhr_raw] Epoch 005 | Loss = 360.2091\n",
            "[fhr_raw] Epoch 006 | Loss = 350.1082\n",
            "[fhr_raw] Epoch 007 | Loss = 335.8103\n",
            "[fhr_raw] Epoch 008 | Loss = 321.7970\n",
            "[fhr_raw] Epoch 009 | Loss = 303.9779\n",
            "[fhr_raw] Epoch 010 | Loss = 285.6000\n",
            "[fhr_raw] Epoch 011 | Loss = 267.0906\n",
            "[fhr_raw] Epoch 012 | Loss = 242.9390\n",
            "[fhr_raw] Epoch 013 | Loss = 225.2319\n",
            "[fhr_raw] Epoch 014 | Loss = 210.1743\n",
            "[fhr_raw] Epoch 015 | Loss = 183.5184\n",
            "[fhr_raw] Epoch 016 | Loss = 168.8533\n",
            "[fhr_raw] Epoch 017 | Loss = 155.2478\n",
            "[fhr_raw] Epoch 018 | Loss = 145.2103\n",
            "[fhr_raw] Epoch 019 | Loss = 133.2300\n",
            "[fhr_raw] Epoch 020 | Loss = 123.7458\n",
            "[fhr_raw] Epoch 021 | Loss = 120.1226\n",
            "[fhr_raw] Epoch 022 | Loss = 116.9158\n",
            "[fhr_raw] Epoch 023 | Loss = 115.5585\n",
            "[fhr_raw] Epoch 024 | Loss = 106.1356\n",
            "[fhr_raw] Epoch 025 | Loss = 111.5364\n",
            "[fhr_raw] Epoch 026 | Loss = 101.9044\n",
            "[fhr_raw] Epoch 027 | Loss = 100.8553\n",
            "[fhr_raw] Epoch 028 | Loss = 95.1752\n",
            "[fhr_raw] Epoch 029 | Loss = 98.9408\n",
            "[fhr_raw] Epoch 030 | Loss = 96.2651\n",
            "[fhr_raw] Epoch 031 | Loss = 91.9250\n",
            "[fhr_raw] Epoch 032 | Loss = 96.0933\n",
            "[fhr_raw] Epoch 033 | Loss = 90.4138\n",
            "[fhr_raw] Epoch 034 | Loss = 88.9868\n",
            "[fhr_raw] Epoch 035 | Loss = 92.2361\n",
            "[fhr_raw] Epoch 036 | Loss = 90.4535\n",
            "[fhr_raw] Epoch 037 | Loss = 86.2131\n",
            "[fhr_raw] Epoch 038 | Loss = 85.1168\n",
            "[fhr_raw] Epoch 039 | Loss = 82.3635\n",
            "[fhr_raw] Epoch 040 | Loss = 81.9574\n",
            "\n",
            "[Fold 4] Extract embeddings (uc_raw & fhr_raw)\n",
            "[Fold 4] X_tr=(3072, 212), X_te=(589, 212)\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006533 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 40897\n",
            "[LightGBM] [Info] Number of data points in the train set: 3072, number of used features: 199\n",
            "[LightGBM] [Info] Start training from score 26.252124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Train MAE = 0.027\n",
            "[Fold 4] Test  MAE = 9.406, RMSE = 12.082, R² = 0.541\n",
            "[Fold 4] Test MAE by gestational week:\n",
            "  Week 30: MAE=28.484 (n=26)\n",
            "  Week 31: MAE=17.491 (n=33)\n",
            "  Week 32: MAE=12.392 (n=32)\n",
            "  Week 33: MAE=13.975 (n=37)\n",
            "  Week 34: MAE=7.093 (n=48)\n",
            "  Week 35: MAE=8.281 (n=73)\n",
            "  Week 36: MAE=9.245 (n=61)\n",
            "  Week 37: MAE=8.492 (n=93)\n",
            "  Week 38: MAE=6.848 (n=99)\n",
            "  Week 39: MAE=4.872 (n=54)\n",
            "  Week 40: MAE=2.330 (n=25)\n",
            "  Week 41: MAE=1.358 (n=8)\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/fold_04_week_mae.csv\n",
            "\n",
            "===== Fold 5/5 =====\n",
            "Train : 2930 from 46 patients\n",
            "Test  : 731 from 11 patients\n",
            "\n",
            "[Fold 5] Train encoder for uc_raw\n",
            "[uc_raw] Epoch 001 | Loss = 718.2447\n",
            "[uc_raw] Epoch 002 | Loss = 426.2859\n",
            "[uc_raw] Epoch 003 | Loss = 391.4116\n",
            "[uc_raw] Epoch 004 | Loss = 364.7944\n",
            "[uc_raw] Epoch 005 | Loss = 345.9159\n",
            "[uc_raw] Epoch 006 | Loss = 332.1648\n",
            "[uc_raw] Epoch 007 | Loss = 318.4395\n",
            "[uc_raw] Epoch 008 | Loss = 312.1371\n",
            "[uc_raw] Epoch 009 | Loss = 303.4573\n",
            "[uc_raw] Epoch 010 | Loss = 290.1302\n",
            "[uc_raw] Epoch 011 | Loss = 281.0513\n",
            "[uc_raw] Epoch 012 | Loss = 269.4032\n",
            "[uc_raw] Epoch 013 | Loss = 254.1782\n",
            "[uc_raw] Epoch 014 | Loss = 249.7590\n",
            "[uc_raw] Epoch 015 | Loss = 237.9594\n",
            "[uc_raw] Epoch 016 | Loss = 215.4392\n",
            "[uc_raw] Epoch 017 | Loss = 202.9192\n",
            "[uc_raw] Epoch 018 | Loss = 184.6607\n",
            "[uc_raw] Epoch 019 | Loss = 168.8302\n",
            "[uc_raw] Epoch 020 | Loss = 150.5112\n",
            "[uc_raw] Epoch 021 | Loss = 134.9345\n",
            "[uc_raw] Epoch 022 | Loss = 124.1635\n",
            "[uc_raw] Epoch 023 | Loss = 114.2652\n",
            "[uc_raw] Epoch 024 | Loss = 109.3444\n",
            "[uc_raw] Epoch 025 | Loss = 96.7533\n",
            "[uc_raw] Epoch 026 | Loss = 93.8197\n",
            "[uc_raw] Epoch 027 | Loss = 86.1951\n",
            "[uc_raw] Epoch 028 | Loss = 82.5435\n",
            "[uc_raw] Epoch 029 | Loss = 77.0338\n",
            "[uc_raw] Epoch 030 | Loss = 76.2909\n",
            "[uc_raw] Epoch 031 | Loss = 76.0929\n",
            "[uc_raw] Epoch 032 | Loss = 77.1552\n",
            "[uc_raw] Epoch 033 | Loss = 72.4073\n",
            "[uc_raw] Epoch 034 | Loss = 72.6221\n",
            "[uc_raw] Epoch 035 | Loss = 74.1900\n",
            "[uc_raw] Epoch 036 | Loss = 68.8513\n",
            "[uc_raw] Epoch 037 | Loss = 65.7761\n",
            "[uc_raw] Epoch 038 | Loss = 72.6337\n",
            "[uc_raw] Epoch 039 | Loss = 69.3026\n",
            "[uc_raw] Epoch 040 | Loss = 69.9060\n",
            "\n",
            "[Fold 5] Train encoder for fhr_raw\n",
            "[fhr_raw] Epoch 001 | Loss = 678.8732\n",
            "[fhr_raw] Epoch 002 | Loss = 412.3539\n",
            "[fhr_raw] Epoch 003 | Loss = 397.1702\n",
            "[fhr_raw] Epoch 004 | Loss = 380.4861\n",
            "[fhr_raw] Epoch 005 | Loss = 367.2869\n",
            "[fhr_raw] Epoch 006 | Loss = 355.9178\n",
            "[fhr_raw] Epoch 007 | Loss = 344.9690\n",
            "[fhr_raw] Epoch 008 | Loss = 329.1824\n",
            "[fhr_raw] Epoch 009 | Loss = 317.8509\n",
            "[fhr_raw] Epoch 010 | Loss = 299.8612\n",
            "[fhr_raw] Epoch 011 | Loss = 276.5887\n",
            "[fhr_raw] Epoch 012 | Loss = 257.5359\n",
            "[fhr_raw] Epoch 013 | Loss = 240.8138\n",
            "[fhr_raw] Epoch 014 | Loss = 213.1717\n",
            "[fhr_raw] Epoch 015 | Loss = 195.2510\n",
            "[fhr_raw] Epoch 016 | Loss = 180.5476\n",
            "[fhr_raw] Epoch 017 | Loss = 163.1742\n",
            "[fhr_raw] Epoch 018 | Loss = 146.5827\n",
            "[fhr_raw] Epoch 019 | Loss = 137.5321\n",
            "[fhr_raw] Epoch 020 | Loss = 119.0620\n",
            "[fhr_raw] Epoch 021 | Loss = 118.3160\n",
            "[fhr_raw] Epoch 022 | Loss = 116.0280\n",
            "[fhr_raw] Epoch 023 | Loss = 97.3707\n",
            "[fhr_raw] Epoch 024 | Loss = 94.9838\n",
            "[fhr_raw] Epoch 025 | Loss = 95.9305\n",
            "[fhr_raw] Epoch 026 | Loss = 91.1031\n",
            "[fhr_raw] Epoch 027 | Loss = 84.5688\n",
            "[fhr_raw] Epoch 028 | Loss = 84.6067\n",
            "[fhr_raw] Epoch 029 | Loss = 83.9934\n",
            "[fhr_raw] Epoch 030 | Loss = 89.8462\n",
            "[fhr_raw] Epoch 031 | Loss = 82.7060\n",
            "[fhr_raw] Epoch 032 | Loss = 83.5607\n",
            "[fhr_raw] Epoch 033 | Loss = 77.6095\n",
            "[fhr_raw] Epoch 034 | Loss = 75.5227\n",
            "[fhr_raw] Epoch 035 | Loss = 76.3643\n",
            "[fhr_raw] Epoch 036 | Loss = 78.9575\n",
            "[fhr_raw] Epoch 037 | Loss = 78.3647\n",
            "[fhr_raw] Epoch 038 | Loss = 73.9564\n",
            "[fhr_raw] Epoch 039 | Loss = 71.1930\n",
            "[fhr_raw] Epoch 040 | Loss = 73.2602\n",
            "\n",
            "[Fold 5] Extract embeddings (uc_raw & fhr_raw)\n",
            "[Fold 5] X_tr=(2930, 212), X_te=(731, 212)\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007910 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 40873\n",
            "[LightGBM] [Info] Number of data points in the train set: 2930, number of used features: 199\n",
            "[LightGBM] [Info] Start training from score 26.302216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Train MAE = 0.041\n",
            "[Fold 5] Test  MAE = 9.107, RMSE = 12.482, R² = 0.462\n",
            "[Fold 5] Test MAE by gestational week:\n",
            "  Week 29: MAE=16.393 (n=4)\n",
            "  Week 30: MAE=14.779 (n=6)\n",
            "  Week 31: MAE=28.449 (n=14)\n",
            "  Week 32: MAE=19.185 (n=40)\n",
            "  Week 33: MAE=10.979 (n=53)\n",
            "  Week 34: MAE=8.790 (n=69)\n",
            "  Week 35: MAE=8.692 (n=59)\n",
            "  Week 36: MAE=9.449 (n=71)\n",
            "  Week 37: MAE=8.531 (n=80)\n",
            "  Week 38: MAE=8.930 (n=110)\n",
            "  Week 39: MAE=7.078 (n=81)\n",
            "  Week 40: MAE=5.762 (n=93)\n",
            "  Week 41: MAE=3.760 (n=50)\n",
            "  Week 42: MAE=3.323 (n=1)\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/fold_05_week_mae.csv\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/prediction.csv\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/fold_overall_metrics.csv\n",
            "\n",
            "Overall across folds:\n",
            "   fold  train_mae  test_mae  test_rmse   test_r2\n",
            "0     1   0.043475  5.339773   7.089605  0.825643\n",
            "1     2   0.043002  4.449458   5.369974  0.931447\n",
            "2     3   0.051883  5.172239   6.328985  0.897702\n",
            "3     4   0.026547  9.405993  12.081721  0.540674\n",
            "4     5   0.040515  9.107263  12.482032  0.462003\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/week_mae_per_fold.csv\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/week_mae_summary.csv\n",
            "\n",
            "Week-wise MAE summary:\n",
            "    week   mae_mean    mae_std  n_total  n_folds\n",
            "0     29  11.153948   6.987010       25        3\n",
            "1     30  14.036322  10.423892       72        4\n",
            "2     31  11.748866  11.066739      105        5\n",
            "3     32   8.861201   6.905164      157        5\n",
            "4     33   8.329298   4.004239      254        5\n",
            "5     34   6.910378   1.151380      313        5\n",
            "6     35   6.768480   1.589332      371        5\n",
            "7     36   7.376461   2.031479      443        5\n",
            "8     37   6.654500   1.922992      503        5\n",
            "9     38   5.695425   2.251245      544        5\n",
            "10    39   4.940578   1.294894      463        5\n",
            "11    40   3.107289   1.537345      321        5\n",
            "12    41   1.628434   1.205472       89        5\n",
            "13    42   3.323105        NaN        1        1\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/week_mae_bar_with_sd.png\n",
            "[Saved] /content/drive/MyDrive/ts2vec-05-yuntao/cv_true_vs_pred_scatter.png\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "7f4fca89",
      "metadata": {
        "id": "7f4fca89"
      },
      "source": [
        "## add\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "7f17b74c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7f17b74c",
        "outputId": "156119d4-b7c5-4905-e8e7-041c09970a63"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pywt\n",
        "\n",
        "# =======================\n",
        "# Config\n",
        "# =======================\n",
        "target_var = \"add\"   # 'add', 'onset', or 'hist'\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "\n",
        "BATCH_SIZE_ENC = 64       # CNN+BiLSTM encoder batch size\n",
        "EPOCHS_ENC = 40           # encoder training epochs\n",
        "LR_ENC = 1e-3             # encoder learning rate\n",
        "\n",
        "D_MODEL = 64              # CNN/BiLSTM feature dim\n",
        "GA_EMB_DIM = 16           # GA MLP embedding dim\n",
        "DROPOUT = 0.20\n",
        "\n",
        "WAVELET_NAME = \"db4\"\n",
        "WAVELET_LEVEL = 3         # raw + D1..D3 + A3 = 5 channels\n",
        "\n",
        "N_FOLDS = 5\n",
        "\n",
        "OUT_DIR = r\"F:\\E3A Healthcare\\TS2VEC+LGBM\\cv_swt_cnn_bilstm_ga_lgbm_5fold_add\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# =======================\n",
        "# Load data\n",
        "# =======================\n",
        "\n",
        "DATA_ROOT = r\"F:\\E3A Healthcare\\TS2VEC+LGBM\\datasets\"\n",
        "json_path = os.path.join(DATA_ROOT, f\"dataset_{target_var}.json\")\n",
        "\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame.from_records(data)\n",
        "print(len(df), \"Measurements (raw)\")\n",
        "\n",
        "# =======================\n",
        "# Basic cleaning & static imputation\n",
        "# =======================\n",
        "\n",
        "age_mean = np.mean([i[0] for i in df[\"static\"] if pd.notna(i[0])])\n",
        "bmi_mean = np.mean([i[1] for i in df[\"static\"] if pd.notna(i[1])])\n",
        "\n",
        "cleaned_data = []\n",
        "for _, m in enumerate(data):\n",
        "\n",
        "    # Remove measurements with empty windows\n",
        "    if len(m[\"uc_windows\"]) == 0 or len(m[\"fhr_windows\"]) == 0:\n",
        "        continue\n",
        "\n",
        "    # Handle NaN values in static\n",
        "    static = m[\"static\"].copy()\n",
        "    if pd.isna(static[0]):\n",
        "        static[0] = age_mean\n",
        "    if pd.isna(static[1]):\n",
        "        static[1] = bmi_mean\n",
        "\n",
        "    copy_m = m.copy()\n",
        "    copy_m[\"static\"] = static\n",
        "    cleaned_data.append(copy_m)\n",
        "\n",
        "cleaned_df = pd.DataFrame(cleaned_data)\n",
        "print(len(cleaned_df), \"Cleaned Measurements\")\n",
        "\n",
        "# 假定 static 最后一个元素是 gest_age_days\n",
        "cleaned_df[\"gest_age_weeks\"] = [(i[-1] // 7) + 1 for i in cleaned_df[\"static\"]]\n",
        "\n",
        "base_records = cleaned_df.to_dict(orient=\"records\")  # 原始不带任何 fold 修改的版本\n",
        "N = len(base_records)\n",
        "all_weeks = cleaned_df[\"gest_age_weeks\"].to_numpy()\n",
        "\n",
        "print(\"Gestational age weeks distribution:\")\n",
        "print(cleaned_df[\"gest_age_weeks\"].value_counts().sort_index())\n",
        "\n",
        "# =======================\n",
        "# SWT helper\n",
        "# =======================\n",
        "\n",
        "def sproc_wavelet_channels(x_1d: np.ndarray):\n",
        "    \"\"\"\n",
        "    SWT(db4, level=3) -> 通道: [raw, D1, D2, D3, A3]\n",
        "    \"\"\"\n",
        "    x_1d = np.asarray(x_1d, dtype=np.float32)\n",
        "    coeffs = pywt.swt(\n",
        "        x_1d, wavelet=WAVELET_NAME,\n",
        "        level=WAVELET_LEVEL,\n",
        "        trim_approx=False,\n",
        "        norm=True\n",
        "    )\n",
        "    # coeffs[l] = (cA_l, cD_l)\n",
        "    chans = [x_1d.astype(np.float32)]  # raw\n",
        "    for l in range(WAVELET_LEVEL):\n",
        "        cA, cD = coeffs[l]\n",
        "        chans.append(cD.astype(np.float32))   # D1..D3\n",
        "    chans.append(coeffs[-1][0].astype(np.float32))  # A3\n",
        "    X = np.stack(chans, axis=0)  # (C, T)\n",
        "\n",
        "    # 逐通道 robust 标准化\n",
        "    X = (X - np.median(X, axis=1, keepdims=True)) / (\n",
        "        np.std(X, axis=1, keepdims=True) + 1e-6\n",
        "    )\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "# =======================\n",
        "# CNN+BiLSTM + GA encoder\n",
        "# =======================\n",
        "\n",
        "class SeqGADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for single signal (uc_raw or fhr_raw) + GA + target\n",
        "    在 __getitem__ 里做 SWT，输出 (C, T)。\n",
        "    \"\"\"\n",
        "    def __init__(self, measurements, signal_key):\n",
        "        self.measurements = measurements\n",
        "        self.signal_key = signal_key\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.measurements)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        m = self.measurements[idx]\n",
        "\n",
        "        seq = np.asarray(m[self.signal_key], dtype=np.float32)  # (T,)\n",
        "        x_ch = sproc_wavelet_channels(seq)                       # (C, T)\n",
        "\n",
        "        y = np.float32(m[\"target\"])\n",
        "        ga = np.float32(m[\"gest_age_weeks\"])  # gest age in weeks\n",
        "\n",
        "        return torch.from_numpy(x_ch), torch.tensor(y), torch.tensor(ga)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, d=1, p=0.0, groups_gn=8):\n",
        "        super().__init__()\n",
        "        pad = (k - 1) // 2 * d\n",
        "        self.conv = nn.Conv1d(\n",
        "            c_in, c_out, kernel_size=k, dilation=d,\n",
        "            padding=pad, bias=False\n",
        "        )\n",
        "        self.gn = nn.GroupNorm(num_groups=min(groups_gn, c_out),\n",
        "                               num_channels=c_out)\n",
        "        self.act = nn.SiLU()\n",
        "        self.dp = nn.Dropout(p)\n",
        "        self.res = (c_in == c_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.dp(self.act(self.gn(self.conv(x))))\n",
        "        return y + x if self.res else y\n",
        "\n",
        "\n",
        "class CNNBiLSTM_GA_Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    结构参考你原来的 CNNBiLSTM_GA_Reg：\n",
        "    - stem Conv1d\n",
        "    - 多层 ConvBlock (dilation k=9,7,5, d=1,2,4)\n",
        "    - BiLSTM\n",
        "    - GA MLP\n",
        "    - 输出:\n",
        "        pred: (B,)\n",
        "        feat: (B, D_MODEL + GA_EMB_DIM)\n",
        "    输入:\n",
        "        x:  (B, C, T)  SWT 通道\n",
        "        ga: (B,)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=5, d_model=64, ga_emb_dim=16, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.ga_emb_dim = ga_emb_dim\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, d_model, kernel_size=7, padding=3, bias=False),\n",
        "            nn.GroupNorm(num_groups=8, num_channels=d_model),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        self.cnn_blocks = nn.Sequential(\n",
        "            ConvBlock(d_model, d_model, k=9, d=1, p=dropout),\n",
        "            ConvBlock(d_model, d_model, k=7, d=2, p=dropout),\n",
        "            ConvBlock(d_model, d_model, k=5, d=4, p=dropout),\n",
        "            nn.Conv1d(d_model, d_model, kernel_size=1, bias=False),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model // 2,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        self.pre_pool_dp = nn.Dropout(dropout)\n",
        "\n",
        "        self.ga_mlp = nn.Sequential(\n",
        "            nn.Linear(1, 16),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(16, ga_emb_dim),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        fuse_dim = d_model + ga_emb_dim\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(fuse_dim, fuse_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fuse_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, ga_scalar):\n",
        "        \"\"\"\n",
        "        x:  (B, C, T)\n",
        "        ga_scalar: (B,)\n",
        "        \"\"\"\n",
        "        f = self.stem(x)            # (B, d_model, T)\n",
        "        f = self.cnn_blocks(f)      # (B, d_model, T)\n",
        "\n",
        "        # BiLSTM\n",
        "        f_t = f.transpose(1, 2)     # (B, T, d_model)\n",
        "        y, _ = self.bilstm(f_t)     # (B, T, d_model)\n",
        "        y = self.pre_pool_dp(y)\n",
        "\n",
        "        # 时间维 average pooling\n",
        "        feat_seq = y.mean(dim=1)    # (B, d_model)\n",
        "\n",
        "        # GA embedding\n",
        "        ga_emb = self.ga_mlp(ga_scalar.unsqueeze(1))  # (B, ga_emb_dim)\n",
        "\n",
        "        fused = torch.cat([feat_seq, ga_emb], dim=1)  # (B, d_model + ga_emb_dim)\n",
        "        pred = self.head(fused).squeeze(-1)           # (B,)\n",
        "\n",
        "        return pred, fused\n",
        "\n",
        "\n",
        "def train_ga_encoder(measurements, signal_key):\n",
        "    \"\"\"\n",
        "    在 train 集上训练一个 SWT+CNN+BiLSTM+GA encoder（监督回归），返回训练好的模型。\n",
        "    \"\"\"\n",
        "    dataset = SeqGADataset(measurements, signal_key)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE_ENC,\n",
        "                        shuffle=True, drop_last=False)\n",
        "\n",
        "    model = CNNBiLSTM_GA_Encoder(\n",
        "        in_ch=5,\n",
        "        d_model=D_MODEL,\n",
        "        ga_emb_dim=GA_EMB_DIM,\n",
        "        dropout=DROPOUT,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR_ENC)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1, EPOCHS_ENC + 1):\n",
        "        epoch_loss = 0.0\n",
        "        n_samples = 0\n",
        "        for x_ch, y, ga in loader:\n",
        "            x_ch = x_ch.to(DEVICE)      # (B, C, T)\n",
        "            y = y.to(DEVICE)            # (B,)\n",
        "            ga = ga.to(DEVICE)          # (B,)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred, _ = model(x_ch, ga)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = x_ch.size(0)\n",
        "            epoch_loss += loss.item() * bs\n",
        "            n_samples += bs\n",
        "\n",
        "        print(f\"[{signal_key}] Epoch {epoch:03d} | Loss = {epoch_loss / n_samples:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def extract_ga_features(measurements, signal_key, model):\n",
        "    \"\"\"\n",
        "    用训练好的 encoder 抽取融合 GA 的特征。\n",
        "    返回 (N, d_model + ga_emb_dim) 的 numpy 数组。\n",
        "    \"\"\"\n",
        "    dataset = SeqGADataset(measurements, signal_key)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE_ENC,\n",
        "                        shuffle=False, drop_last=False)\n",
        "\n",
        "    model.eval()\n",
        "    feats = []\n",
        "    with torch.no_grad():\n",
        "        for x_ch, y, ga in loader:\n",
        "            x_ch = x_ch.to(DEVICE)       # (B, C, T)\n",
        "            ga = ga.to(DEVICE)\n",
        "            _, feat = model(x_ch, ga)    # (B, d_model + ga_emb_dim)\n",
        "            feats.append(feat.cpu().numpy())\n",
        "\n",
        "    feats = np.concatenate(feats, axis=0)\n",
        "    return feats\n",
        "\n",
        "# =======================\n",
        "# 5-fold cross-validation\n",
        "# =======================\n",
        "\n",
        "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "all_true = []\n",
        "all_pred = []\n",
        "all_weeks_pred = []\n",
        "fold_metrics = []\n",
        "rows_week_fold = []\n",
        "all_pred_rows = []   # <<< NEW: 用于保存所有 test 样本的预测\n",
        "\n",
        "indices = np.arange(N)\n",
        "\n",
        "for fold, (tr_idx, te_idx) in enumerate(kf.split(indices), start=1):\n",
        "    print(f\"\\n===== Fold {fold}/{N_FOLDS} =====\")\n",
        "\n",
        "    # 每个 fold 用 base_records 的深拷贝，避免之前的修改污染\n",
        "    train_recs = [copy.deepcopy(base_records[i]) for i in tr_idx]\n",
        "    test_recs = [copy.deepcopy(base_records[i]) for i in te_idx]\n",
        "\n",
        "    print(f\"Train samples: {len(train_recs)}, Test samples: {len(test_recs)}\")\n",
        "\n",
        "    # --- 1. 训练两个 encoder ---\n",
        "    print(\"\\n[Fold %d] Train encoder for uc_raw\" % fold)\n",
        "    encoder_uc = train_ga_encoder(train_recs, \"uc_raw\")\n",
        "\n",
        "    print(\"\\n[Fold %d] Train encoder for fhr_raw\" % fold)\n",
        "    encoder_fhr = train_ga_encoder(train_recs, \"fhr_raw\")\n",
        "\n",
        "    # --- 2. 提取 embedding ---\n",
        "    print(\"\\n[Fold %d] Extract embeddings (uc_raw & fhr_raw)\" % fold)\n",
        "    train_uc_embed = extract_ga_features(train_recs, \"uc_raw\", encoder_uc)\n",
        "    train_fhr_embed = extract_ga_features(train_recs, \"fhr_raw\", encoder_fhr)\n",
        "    test_uc_embed = extract_ga_features(test_recs, \"uc_raw\", encoder_uc)\n",
        "    test_fhr_embed = extract_ga_features(test_recs, \"fhr_raw\", encoder_fhr)\n",
        "\n",
        "    for i, e in enumerate(train_uc_embed):\n",
        "        train_recs[i][\"uc_raw\"] = e\n",
        "    for i, e in enumerate(train_fhr_embed):\n",
        "        train_recs[i][\"fhr_raw\"] = e\n",
        "    for i, e in enumerate(test_uc_embed):\n",
        "        test_recs[i][\"uc_raw\"] = e\n",
        "    for i, e in enumerate(test_fhr_embed):\n",
        "        test_recs[i][\"fhr_raw\"] = e\n",
        "\n",
        "    # --- 3. 聚合 window-level 特征 ---\n",
        "    for rec in train_recs:\n",
        "        uc_ws = rec[\"uc_windows\"]\n",
        "        fhr_ws = rec[\"fhr_windows\"]\n",
        "        # 原始是 list[dict]；这里才做 mean pooling\n",
        "        uc_w = np.array([[v for _, v in w.items()] for w in uc_ws],\n",
        "                        dtype=np.float32)\n",
        "        fhr_w = np.array([[v for _, v in w.items()] for w in fhr_ws],\n",
        "                         dtype=np.float32)\n",
        "        rec[\"uc_windows\"] = uc_w.mean(axis=0)\n",
        "        rec[\"fhr_windows\"] = fhr_w.mean(axis=0)\n",
        "\n",
        "    for rec in test_recs:\n",
        "        uc_ws = rec[\"uc_windows\"]\n",
        "        fhr_ws = rec[\"fhr_windows\"]\n",
        "        uc_w = np.array([[v for _, v in w.items()] for w in uc_ws],\n",
        "                        dtype=np.float32)\n",
        "        fhr_w = np.array([[v for _, v in w.items()] for w in fhr_ws],\n",
        "                         dtype=np.float32)\n",
        "        rec[\"uc_windows\"] = uc_w.mean(axis=0)\n",
        "        rec[\"fhr_windows\"] = fhr_w.mean(axis=0)\n",
        "\n",
        "    # 维度信息\n",
        "    FUSED_DIM = len(train_recs[0][\"uc_raw\"])        # 64 + 16 = 80\n",
        "    STATIC_DIM = len(train_recs[0][\"static\"])\n",
        "    UC_WIN_DIM = len(train_recs[0][\"uc_windows\"])   # 20\n",
        "    FHR_WIN_DIM = len(train_recs[0][\"fhr_windows\"]) # 24\n",
        "\n",
        "    FEATURE_ORDER = [\"uc_raw\", \"fhr_raw\", \"fhr_windows\", \"uc_windows\", \"static\"]\n",
        "    FEATURE_DIMS = {\n",
        "        \"uc_raw\": FUSED_DIM,\n",
        "        \"fhr_raw\": FUSED_DIM,\n",
        "        \"fhr_windows\": FHR_WIN_DIM,\n",
        "        \"uc_windows\": UC_WIN_DIM,\n",
        "        \"static\": STATIC_DIM,\n",
        "    }\n",
        "\n",
        "    def to_feature_vec(m):\n",
        "        parts = []\n",
        "        for k in FEATURE_ORDER:\n",
        "            v = np.asarray(m[k], dtype=np.float32).ravel()\n",
        "            assert v.size == FEATURE_DIMS[k], f\"{k} dim mismatch: {v.size} vs {FEATURE_DIMS[k]}\"\n",
        "            parts.append(v)\n",
        "        x = np.concatenate(parts).astype(np.float32, copy=False)\n",
        "        return x\n",
        "\n",
        "    def make_xy(measurements):\n",
        "        X = np.stack([to_feature_vec(m) for m in measurements], axis=0)\n",
        "        y = np.asarray([m[\"target\"] for m in measurements], dtype=np.float32).ravel()\n",
        "        weeks = np.asarray([m[\"gest_age_weeks\"] for m in measurements], dtype=np.int32)\n",
        "        return X, y, weeks\n",
        "\n",
        "    X_tr, y_tr, weeks_tr = make_xy(train_recs)\n",
        "    X_te, y_te, weeks_te = make_xy(test_recs)\n",
        "\n",
        "    print(f\"[Fold {fold}] X_tr={X_tr.shape}, X_te={X_te.shape}\")\n",
        "\n",
        "    # --- 4. 训练 LGBM ---\n",
        "    lgbm = LGBMRegressor(\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    lgbm.fit(X_tr, y_tr)\n",
        "\n",
        "    # --- 5. 预测 & 评估 ---\n",
        "    y_tr_pred = lgbm.predict(X_tr)\n",
        "    y_te_pred = lgbm.predict(X_te)\n",
        "\n",
        "    fold_train_mae = mean_absolute_error(y_tr, y_tr_pred)\n",
        "    fold_test_mae = mean_absolute_error(y_te, y_te_pred)\n",
        "    fold_test_rmse = np.sqrt(mean_squared_error(y_te, y_te_pred))\n",
        "    fold_test_r2 = r2_score(y_te, y_te_pred)\n",
        "\n",
        "    print(f\"[Fold {fold}] Train MAE = {fold_train_mae:.3f}\")\n",
        "    print(f\"[Fold {fold}] Test  MAE = {fold_test_mae:.3f}, RMSE = {fold_test_rmse:.3f}, R² = {fold_test_r2:.3f}\")\n",
        "\n",
        "    fold_metrics.append({\n",
        "        \"fold\": fold,\n",
        "        \"train_mae\": fold_train_mae,\n",
        "        \"test_mae\": fold_test_mae,\n",
        "        \"test_rmse\": fold_test_rmse,\n",
        "        \"test_r2\": fold_test_r2,\n",
        "    })\n",
        "\n",
        "    # 记录全体散点用\n",
        "    all_true.append(y_te)\n",
        "    all_pred.append(y_te_pred)\n",
        "    all_weeks_pred.append(weeks_te)\n",
        "\n",
        "    # <<< NEW: 保存当前 fold 的所有 test 样本的 actual / predicted\n",
        "    df_pred_fold = pd.DataFrame({\n",
        "        \"fold\": fold,\n",
        "        \"gest_age_weeks\": weeks_te.astype(int),\n",
        "        \"y_true\": y_te.astype(float),\n",
        "        \"y_pred\": y_te_pred.astype(float),\n",
        "    })\n",
        "    all_pred_rows.append(df_pred_fold)\n",
        "    # >>> END NEW\n",
        "\n",
        "    # --- 6. 每周 MAE ---\n",
        "    abs_err = np.abs(y_te_pred - y_te)\n",
        "    rows_this_fold = []\n",
        "    print(f\"[Fold {fold}] Test MAE by gestational week:\")\n",
        "    for w in sorted(np.unique(weeks_te)):\n",
        "        mask_w = (weeks_te == w)\n",
        "        n_w = int(mask_w.sum())\n",
        "        if n_w == 0:\n",
        "            continue\n",
        "        mae_w = float(abs_err[mask_w].mean())\n",
        "        rows_this_fold.append({\n",
        "            \"fold\": fold,\n",
        "            \"week\": int(w),\n",
        "            \"n_samples\": n_w,\n",
        "            \"mae\": mae_w,\n",
        "        })\n",
        "        print(f\"  Week {int(w):2d}: MAE={mae_w:.3f} (n={n_w})\")\n",
        "\n",
        "    # 加到总表\n",
        "    rows_week_fold.extend(rows_this_fold)\n",
        "\n",
        "    # 每折单独保存一个 per-week CSV（可选）\n",
        "    df_fold_week = pd.DataFrame(rows_this_fold)\n",
        "    fold_week_csv = os.path.join(OUT_DIR, f\"fold_{fold:02d}_week_mae.csv\")\n",
        "    df_fold_week.to_csv(fold_week_csv, index=False)\n",
        "    print(f\"[Saved] {fold_week_csv}\")\n",
        "\n",
        "# =======================\n",
        "# 汇总 prediction.csv\n",
        "# =======================\n",
        "\n",
        "df_pred_all = pd.concat(all_pred_rows, ignore_index=True)   # <<< NEW\n",
        "pred_csv_path = os.path.join(OUT_DIR, \"prediction.csv\")     # <<< NEW\n",
        "df_pred_all.to_csv(pred_csv_path, index=False)              # <<< NEW\n",
        "print(f\"[Saved] {pred_csv_path}\")                           # <<< NEW\n",
        "\n",
        "# =======================\n",
        "# 汇总结果\n",
        "# =======================\n",
        "\n",
        "all_true = np.concatenate(all_true)\n",
        "all_pred = np.concatenate(all_pred)\n",
        "all_weeks_pred = np.concatenate(all_weeks_pred)\n",
        "\n",
        "# 每折整体性能表\n",
        "df_folds = pd.DataFrame(fold_metrics)\n",
        "fold_summary_csv = os.path.join(OUT_DIR, \"fold_overall_metrics.csv\")\n",
        "df_folds.to_csv(fold_summary_csv, index=False)\n",
        "print(f\"[Saved] {fold_summary_csv}\")\n",
        "\n",
        "print(\"\\nOverall across folds:\")\n",
        "print(df_folds)\n",
        "\n",
        "# 每折 × 每周表\n",
        "df_week_fold = pd.DataFrame(rows_week_fold)\n",
        "week_fold_csv = os.path.join(OUT_DIR, \"week_mae_per_fold.csv\")\n",
        "df_week_fold.to_csv(week_fold_csv, index=False)\n",
        "print(f\"[Saved] {week_fold_csv}\")\n",
        "\n",
        "# 按 week 汇总 mean / std\n",
        "df_week_summary = df_week_fold.groupby(\"week\").agg(\n",
        "    mae_mean=(\"mae\", \"mean\"),\n",
        "    mae_std=(\"mae\", \"std\"),\n",
        "    n_total=(\"n_samples\", \"sum\"),\n",
        "    n_folds=(\"mae\", \"count\"),\n",
        ").reset_index()\n",
        "\n",
        "week_summary_csv = os.path.join(OUT_DIR, \"week_mae_summary.csv\")\n",
        "df_week_summary.to_csv(week_summary_csv, index=False)\n",
        "print(f\"[Saved] {week_summary_csv}\")\n",
        "\n",
        "print(\"\\nWeek-wise MAE summary:\")\n",
        "print(df_week_summary)\n",
        "\n",
        "# =======================\n",
        "# 可视化 1：bar chart with SD (per week MAE)\n",
        "# =======================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "x = df_week_summary[\"week\"].values\n",
        "y = df_week_summary[\"mae_mean\"].values\n",
        "yerr = df_week_summary[\"mae_std\"].values\n",
        "\n",
        "plt.bar(x, y, yerr=yerr, capsize=3)\n",
        "plt.xlabel(\"Gestational Age (weeks)\")\n",
        "plt.ylabel(\"MAE (days-to-onset)\")\n",
        "plt.title(\"Per-week MAE across 5-fold CV (mean ± SD)\")\n",
        "plt.xticks(x)\n",
        "plt.tight_layout()\n",
        "bar_path = os.path.join(OUT_DIR, \"week_mae_bar_with_sd.png\")\n",
        "plt.savefig(bar_path, dpi=200)\n",
        "plt.close()\n",
        "print(f\"[Saved] {bar_path}\")\n",
        "\n",
        "# =======================\n",
        "# 可视化 2：拟合散点图 (true vs pred)\n",
        "# =======================\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(all_true, all_pred, s=8, alpha=0.5)\n",
        "mn = float(min(all_true.min(), all_pred.min()))\n",
        "mx = float(max(all_true.max(), all_pred.max()))\n",
        "plt.plot([mn, mx], [mn, mx], \"r--\", label=\"Ideal\")\n",
        "\n",
        "overall_mae = mean_absolute_error(all_true, all_pred)\n",
        "overall_rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
        "overall_r2 = r2_score(all_true, all_pred)\n",
        "\n",
        "plt.xlabel(\"True days to onset\")\n",
        "plt.ylabel(\"Predicted days to onset\")\n",
        "plt.title(f\"5-fold CV — True vs Pred\\nMAE={overall_mae:.2f}, RMSE={overall_rmse:.2f}, R²={overall_r2:.3f}\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "scatter_path = os.path.join(OUT_DIR, \"cv_true_vs_pred_scatter.png\")\n",
        "plt.savefig(scatter_path, dpi=200)\n",
        "plt.close()\n",
        "print(f\"[Saved] {scatter_path}\")\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'F:\\\\E3A Healthcare\\\\TS2VEC+LGBM\\\\datasets/dataset_add.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3557490376.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mjson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"dataset_{target_var}.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\E3A Healthcare\\\\TS2VEC+LGBM\\\\datasets/dataset_add.json'"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2e925f95d5eb196e",
        "2596a51d53919e85",
        "2b80d53bda529f02"
      ],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}