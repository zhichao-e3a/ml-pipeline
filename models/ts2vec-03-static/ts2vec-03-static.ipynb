{
  "cells": [
    {
      "metadata": {
        "id": "4b0abec620d7b90b"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount Storage, Import Libraries"
      ],
      "id": "4b0abec620d7b90b"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:13:53.519707Z",
          "start_time": "2025-11-11T08:13:49.896634Z"
        },
        "id": "ca08b309416f9cbc"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "id": "ca08b309416f9cbc",
      "outputs": [],
      "execution_count": 224
    },
    {
      "cell_type": "code",
      "source": [
        "target_var = 'onset'\n",
        "n_epochs   = 200"
      ],
      "metadata": {
        "id": "oyaov3ZP2I4H"
      },
      "id": "oyaov3ZP2I4H",
      "execution_count": 225,
      "outputs": []
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-11-11T08:13:54.404633Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a55b7fa8ff26a7ef",
        "outputId": "6c395e29-ab09-4217-b2f2-6ec01bdfc6bc"
      },
      "cell_type": "code",
      "source": [
        "########## For Colab ##########\n",
        "!pip install ts2vec\n",
        "from ts2vec import TS2Vec\n",
        "\n",
        "########## Personal ##########\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open(f'/content/drive/MyDrive/datasets/dataset_{target_var}.json') as f:\n",
        "    content = f.read()\n",
        "    data = json.loads(content)\n",
        "\n",
        "########## Enterprise ##########\n",
        "# import gcsfs\n",
        "# fs = gcsfs.GCSFileSystem()\n",
        "# with fs.open('gs://modoo-eod/users/datasets/dataset_hist.json') as f:\n",
        "#     content = f.read()\n",
        "#     data = json.loads(content)\n",
        "\n",
        "########## Local ##########\n",
        "# with open(\"../../datasets/dataset_hist.json\") as f:\n",
        "#     content=f.read()\n",
        "#     data=json.loads(content)"
      ],
      "id": "a55b7fa8ff26a7ef",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ts2vec in /usr/local/lib/python3.12/dist-packages (0.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": 226
    },
    {
      "metadata": {
        "id": "37aa3927911796ca"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "id": "37aa3927911796ca"
    },
    {
      "metadata": {
        "id": "5801c834c409cc46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe85cab-4e12-44e2-d886-ec6c04decc27"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3681 Measurements\n",
            "3661 Cleaned Measurements\n"
          ]
        }
      ],
      "execution_count": 227,
      "source": [
        "df = pd.DataFrame.from_records(data)\n",
        "\n",
        "print(len(df), \"Measurements\")\n",
        "\n",
        "age_mean = np.mean([i[0] for i in df['static'] if pd.notna(i[0])])\n",
        "bmi_mean = np.mean([i[1] for i in df['static'] if pd.notna(i[1])])\n",
        "\n",
        "cleaned_data = []\n",
        "for _, m in enumerate(data):\n",
        "\n",
        "    # Remove measurements with empty windows\n",
        "    if len(m['uc_windows']) == 0 or len(m['fhr_windows']) == 0:\n",
        "        continue\n",
        "\n",
        "    # Handle NaN values\n",
        "    static = m['static'].copy()\n",
        "    if pd.isna(m['static'][0]):\n",
        "        static[0] = age_mean\n",
        "    if pd.isna(m['static'][1]):\n",
        "        static[1] = bmi_mean\n",
        "\n",
        "    copy = m.copy()\n",
        "    copy['static'] = static\n",
        "    cleaned_data.append(copy)\n",
        "\n",
        "cleaned_df = pd.DataFrame(cleaned_data)\n",
        "cleaned_df[\"gest_age_weeks\"] = [(i[-1]//7)+1 for i in cleaned_df[\"static\"]]\n",
        "\n",
        "print(len(cleaned_df), \"Cleaned Measurements\")"
      ],
      "id": "5801c834c409cc46"
    },
    {
      "metadata": {
        "id": "2047a92029827c31"
      },
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split (Stratified)"
      ],
      "id": "2047a92029827c31"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:08:34.552904Z",
          "start_time": "2025-11-11T08:08:34.551056Z"
        },
        "id": "20a4774983baf870",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd55faaf-8266-4382-eaf8-a3aa8c0ef631"
      },
      "cell_type": "code",
      "source": [
        "df_train = cleaned_df.groupby(\n",
        "    \"gest_age_weeks\",\n",
        "    group_keys=False\n",
        ").apply(lambda x: x.sample(frac=0.8), include_groups=True)\n",
        "\n",
        "df_test = cleaned_df.drop(df_train.index)"
      ],
      "id": "20a4774983baf870",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3841771799.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  ).apply(lambda x: x.sample(frac=0.8), include_groups=True)\n"
          ]
        }
      ],
      "execution_count": 228
    },
    {
      "metadata": {
        "id": "762763f8fb620d5d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 229,
      "source": [
        "train = df_train.to_dict(orient='records')\n",
        "test  = df_test.to_dict(orient='records')"
      ],
      "id": "762763f8fb620d5d"
    },
    {
      "metadata": {
        "id": "aace41117a7a8a4e"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-Compute TS2Vec Embeddings"
      ],
      "id": "aace41117a7a8a4e"
    },
    {
      "metadata": {
        "id": "f886fd5803222e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a4543b-9c22-4fe6-d393-c23d18bb8589"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train has shape (2928, 2048, 1)\n",
            "Test has shape  (733, 2048, 1)\n"
          ]
        }
      ],
      "execution_count": 230,
      "source": [
        "# n_instances x n_timestamps x n_features\n",
        "train_uc = np.expand_dims(np.array([i['uc_raw'] for i in train]), 2)\n",
        "train_fhr = np.expand_dims(np.array([i['fhr_raw'] for i in train]), 2)\n",
        "test_uc  = np.expand_dims(np.array([i['uc_raw'] for i in test]), 2)\n",
        "test_fhr = np.expand_dims(np.array([i['fhr_raw'] for i in test]), 2)\n",
        "\n",
        "print(\"Train has shape\", train_fhr.shape)\n",
        "print(\"Test has shape \", test_fhr.shape)\n",
        "\n",
        "ts_model = TS2Vec(\n",
        "    input_dims = 1,\n",
        "    output_dims = 320,\n",
        "    device = 0,\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "train_uc_embed    = ts_model.encode(train_uc, encoding_window=\"full_series\")\n",
        "train_fhr_embed   = ts_model.encode(train_fhr, encoding_window=\"full_series\")\n",
        "\n",
        "test_uc_embed     = ts_model.encode(test_uc, encoding_window=\"full_series\")\n",
        "test_fhr_embed    = ts_model.encode(test_fhr, encoding_window=\"full_series\")\n",
        "\n",
        "for idx, e in enumerate(train_uc_embed):\n",
        "    train[idx]['uc_raw'] = e\n",
        "\n",
        "for idx, e in enumerate(train_fhr_embed):\n",
        "    train[idx]['fhr_raw'] = e\n",
        "\n",
        "for idx, e in enumerate(test_uc_embed):\n",
        "    test[idx]['uc_raw'] = e\n",
        "\n",
        "for idx, e in enumerate(test_fhr_embed):\n",
        "    test[idx]['fhr_raw'] = e"
      ],
      "id": "f886fd5803222e01"
    },
    {
      "metadata": {
        "id": "2cee68f133eb290"
      },
      "cell_type": "markdown",
      "source": [
        "### Aggregate Windows"
      ],
      "id": "2cee68f133eb290"
    },
    {
      "metadata": {
        "id": "258fc28af32309f8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 231,
      "source": [
        "for i in train:\n",
        "\n",
        "    uc_w    = torch.tensor([[v for _, v in w.items()] for w in i['uc_windows']], dtype=torch.float32)\n",
        "    fhr_w   = torch.tensor([[v for _, v in w.items()] for w in i['fhr_windows']],dtype=torch.float32)\n",
        "    uc_mean = uc_w.mean(dim=0) ; fhr_mean = fhr_w.mean(dim=0)\n",
        "\n",
        "    i['uc_windows'] = uc_mean ; i['fhr_windows'] = fhr_mean\n",
        "\n",
        "for i in test:\n",
        "\n",
        "    uc_w    = torch.tensor([[v for _, v in w.items()] for w in i['uc_windows']], dtype=torch.float32)\n",
        "    fhr_w   = torch.tensor([[v for _, v in w.items()] for w in i['fhr_windows']],dtype=torch.float32)\n",
        "    uc_mean = uc_w.mean(dim=0) ; fhr_mean = fhr_w.mean(dim=0)\n",
        "\n",
        "    i['uc_windows'] = uc_mean ; i['fhr_windows'] = fhr_mean"
      ],
      "id": "258fc28af32309f8"
    },
    {
      "metadata": {
        "id": "b6290aa02c91e10a"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset, Data Loader"
      ],
      "id": "b6290aa02c91e10a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:09:58.689558Z",
          "start_time": "2025-11-11T08:09:58.614911Z"
        },
        "id": "c928b2daa872b176"
      },
      "cell_type": "code",
      "source": [
        "class PatientDataset(Dataset):\n",
        "\n",
        "    def __init__(self, measurements):\n",
        "\n",
        "        self.measurements = measurements\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.measurements)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        measurement = self.measurements[idx]\n",
        "\n",
        "        return measurement"
      ],
      "id": "c928b2daa872b176",
      "outputs": [],
      "execution_count": 232
    },
    {
      "metadata": {
        "id": "386849a013919c4e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 233,
      "source": [
        "def patient_collate_fn(batch):\n",
        "\n",
        "    uc_raw  = torch.stack([torch.tensor(m['uc_raw'], dtype=torch.float32) for m in batch])\n",
        "    fhr_raw = torch.stack([torch.tensor(m['fhr_raw'], dtype=torch.float32) for m in batch])\n",
        "    static  = torch.stack([torch.tensor(m['static'], dtype=torch.float32) for m in batch])\n",
        "    target  = torch.stack([torch.tensor(m['target'], dtype=torch.float32) for m in batch])\n",
        "    uc_win  = torch.stack([m['uc_windows'] for m in batch])\n",
        "    fhr_win = torch.stack([m['fhr_windows'] for m in batch])\n",
        "\n",
        "    return {\n",
        "        'uc_raw'      : uc_raw,\n",
        "        'fhr_raw'     : fhr_raw,\n",
        "        'uc_windows'  : uc_win,\n",
        "        'fhr_windows' : fhr_win,\n",
        "        'static'      : static,\n",
        "        'target'      : target\n",
        "    }"
      ],
      "id": "386849a013919c4e"
    },
    {
      "metadata": {
        "id": "2f0218a5492371d5"
      },
      "cell_type": "markdown",
      "source": [
        "### Model Cfg"
      ],
      "id": "2f0218a5492371d5"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:10:31.267495Z",
          "start_time": "2025-11-11T08:10:31.255302Z"
        },
        "id": "b76be0a693ba1cf7"
      },
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelCfg:\n",
        "\n",
        "    # Raw UC/FHR\n",
        "    ts2vec_in_dim   : int = 1\n",
        "    ts2vec_out_dim  : int = 320\n",
        "\n",
        "    # FHR Windows\n",
        "    fhr_in_dim      : int = 24\n",
        "    fhr_hidden_dim  : int = 64\n",
        "    fhr_out_dim     : int = 32\n",
        "\n",
        "    # UC Windows\n",
        "    uc_in_dim       : int = 20\n",
        "    uc_hidden_dim   : int = 64\n",
        "    uc_out_dim      : int = 32\n",
        "\n",
        "    # Static\n",
        "    stat_in_dim     : int = 8\n",
        "    stat_hidden_dim : int = 64\n",
        "    stat_out_dim    : int = 32\n",
        "\n",
        "    # Fused Regressor\n",
        "    fuse_hidden_dim : int = 512"
      ],
      "id": "b76be0a693ba1cf7",
      "outputs": [],
      "execution_count": 234
    },
    {
      "metadata": {
        "id": "3a04c92a5d9806a6"
      },
      "cell_type": "markdown",
      "source": [
        "### Model Modules"
      ],
      "id": "3a04c92a5d9806a6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:10:44.715420Z",
          "start_time": "2025-11-11T08:10:44.705241Z"
        },
        "id": "79b5f04a77fa3ba5"
      },
      "cell_type": "code",
      "source": [
        "class StaticEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # (B, 32)\n",
        "        return self.net(x)"
      ],
      "id": "79b5f04a77fa3ba5",
      "outputs": [],
      "execution_count": 235
    },
    {
      "metadata": {
        "id": "6461b99b10cbb95f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 236,
      "source": [
        "class WindowsEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, window):\n",
        "\n",
        "        # (B, 24|20) -> (B, 32)\n",
        "        h = self.mlp(window)\n",
        "\n",
        "        # (B, 32)\n",
        "        return h"
      ],
      "id": "6461b99b10cbb95f"
    },
    {
      "metadata": {
        "id": "c51c70bcc3e33fa0"
      },
      "cell_type": "markdown",
      "source": [
        "### Main Model Class"
      ],
      "id": "c51c70bcc3e33fa0"
    },
    {
      "metadata": {
        "id": "79410141de4d31f7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 237,
      "source": [
        "class FusedRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg: ModelCfg):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # UC Window Encoder: Output 32\n",
        "        self.uc_win_encoder = WindowsEncoder(\n",
        "            in_dim=self.cfg.uc_in_dim,\n",
        "            hidden_dim=self.cfg.uc_hidden_dim,\n",
        "            out_dim=self.cfg.uc_out_dim,\n",
        "        )\n",
        "\n",
        "        # FHR Window Encoder: Output 32\n",
        "        self.fhr_win_encoder = WindowsEncoder(\n",
        "            in_dim=self.cfg.fhr_in_dim,\n",
        "            hidden_dim=self.cfg.fhr_hidden_dim,\n",
        "            out_dim=self.cfg.fhr_out_dim,\n",
        "        )\n",
        "\n",
        "        # Static Encoder: Output 32\n",
        "        self.static_encoder = StaticEncoder(\n",
        "            in_dim=self.cfg.stat_in_dim,\n",
        "            hidden_dim=self.cfg.stat_hidden_dim,\n",
        "            out_dim=self.cfg.stat_out_dim,\n",
        "        )\n",
        "\n",
        "        fused_dim = (\n",
        "            2 * self.cfg.ts2vec_out_dim # 640\n",
        "            + self.cfg.uc_out_dim       # 32\n",
        "            + self.cfg.fhr_out_dim      # 32\n",
        "            + self.cfg.stat_out_dim     # 32\n",
        "        )\n",
        "\n",
        "        # fused_dim = (\n",
        "        #     2 * self.cfg.ts2vec_out_dim\n",
        "        #     + 20\n",
        "        #     + 24\n",
        "        #     + 8\n",
        "        # )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(fused_dim, self.cfg.fuse_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.cfg.fuse_hidden_dim, self.cfg.fuse_hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.cfg.fuse_hidden_dim//2, self.cfg.fuse_hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.cfg.fuse_hidden_dim//4, self.cfg.fuse_hidden_dim//8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.cfg.fuse_hidden_dim//8, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch):\n",
        "\n",
        "        uc_raw = batch['uc_raw'] ; uc_raw_tensor = uc_raw.to(device)\n",
        "\n",
        "        uc_windows = batch['uc_windows'] ; uc_win_tensor = uc_windows.to(device)\n",
        "\n",
        "        fhr_raw = batch['fhr_raw'] ; fhr_raw_tensor = fhr_raw.to(device)\n",
        "\n",
        "        fhr_windows = batch['fhr_windows'] ; fhr_win_tensor = fhr_windows.to(device)\n",
        "\n",
        "        static = batch['static'] ; static_tensor = static.to(device)\n",
        "\n",
        "        # (B, 20) -> (B, 32)\n",
        "        uc_win_emb = self.uc_win_encoder(uc_win_tensor)\n",
        "        # print(\"UC Windows Shape:\", uc_win_emb.shape)\n",
        "\n",
        "        # (B, 24) -> (B, 32)\n",
        "        fhr_win_emb = self.fhr_win_encoder(fhr_win_tensor)\n",
        "        # print(\"FHR Windows Shape:\", fhr_win_emb.shape)\n",
        "\n",
        "        # (B, 8) -> (B, 32)\n",
        "        static_emb = self.static_encoder(static_tensor)\n",
        "        # print(\"Static Shape:\", static_emb.shape)\n",
        "\n",
        "        # (B, 736)\n",
        "        fused = torch.cat(\n",
        "            [uc_raw_tensor, fhr_raw_tensor, uc_win_emb, fhr_win_emb, static_emb],\n",
        "            dim=-1,\n",
        "        ).to(device)\n",
        "        # print(\"Fused Shape:\", fused.shape)\n",
        "\n",
        "        # (B, 1)\n",
        "        preds = self.fusion(fused)\n",
        "        # print(\"Fusion Output Shape:\", preds.shape)\n",
        "\n",
        "        return preds.squeeze(1)\n"
      ],
      "id": "79410141de4d31f7"
    },
    {
      "metadata": {
        "id": "32979e8feb867cc6"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Eval Functions"
      ],
      "id": "32979e8feb867cc6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:11:55.460525Z",
          "start_time": "2025-11-11T08:11:55.451681Z"
        },
        "id": "c69c42c2368d160c"
      },
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimiser, criterion, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in loader:\n",
        "\n",
        "        target = batch['target'].to(device)\n",
        "        # print(\"Targets Shape:\", target.shape)\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        pred = model(batch)\n",
        "\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimiser.step()\n",
        "\n",
        "        n_batches += 1\n",
        "\n",
        "        # print(\"Train\", pred, target, sep=\"|\")\n",
        "\n",
        "    return total_loss / n_batches"
      ],
      "id": "c69c42c2368d160c",
      "outputs": [],
      "execution_count": 238
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:11:59.485581Z",
          "start_time": "2025-11-11T08:11:59.461861Z"
        },
        "id": "289ee5c6469fcce4"
      },
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    group_loss = defaultdict(list)\n",
        "\n",
        "    for batch in loader:\n",
        "\n",
        "        target = batch['target'].to(device)\n",
        "\n",
        "        pred = model(batch)\n",
        "\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        n_batches += 1\n",
        "\n",
        "        gest_days  = batch['static'][:,-1]\n",
        "        gest_weeks = (gest_days // 7).long()+1\n",
        "\n",
        "        abs_err = (pred-target).abs()\n",
        "\n",
        "        for g, e in zip(gest_weeks.tolist(), abs_err.tolist()):\n",
        "            group_loss[str(g)].append(e)\n",
        "\n",
        "    per_group_mae = {\n",
        "        g: round(float(torch.tensor(es).mean()), 4)\\\n",
        "        for g, es in sorted(group_loss.items(), key=lambda x: x[0])\n",
        "    }\n",
        "\n",
        "    return total_loss/n_batches, per_group_mae"
      ],
      "id": "289ee5c6469fcce4",
      "outputs": [],
      "execution_count": 239
    },
    {
      "metadata": {
        "id": "6b33e8451847d2fd"
      },
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "id": "6b33e8451847d2fd"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-11T08:12:09.230181Z",
          "start_time": "2025-11-11T08:12:09.217165Z"
        },
        "id": "9b1aec55272f0c8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "59177ad186a241319682a0521ddde247",
            "18ad063a55504a8ea7cc790828b1bb80",
            "8cf9b36a7e0746f9b013458e269926d7",
            "01362b0c571243ba8f0ef684072ebdc1",
            "5fdbb09a919b4309b6641676fd1df424",
            "a2b81e05ce034576857be1892346d8f0",
            "241e58f74f924657924caf7202e2a1ac",
            "d7f93de2488b4ddaa7a6d4383c3c30af",
            "32c8128e0861409ab9d7767a1d874564",
            "1bec2e9d387946d0871e4e2b324bf72c",
            "5367ba18fc4c49e8846b9f3c3dc7f59a"
          ]
        },
        "outputId": "93089791-f1b4-41fc-8e9c-96ab0701a36c"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "model_cfg = ModelCfg()\n",
        "\n",
        "model = FusedRegressor(model_cfg).to(device)\n",
        "\n",
        "train_dataset = PatientDataset(train)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    collate_fn=patient_collate_fn,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "eval_dataset = PatientDataset(test)\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    eval_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=patient_collate_fn,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()),\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-5\n",
        ")\n",
        "\n",
        "criterion = torch.nn.L1Loss()\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "# {'week' : [err_1, err_2, ...]}\n",
        "group_losses_week = {str(k):[] for k in cleaned_df['gest_age_weeks']}\n",
        "\n",
        "group_losses = []\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "\n",
        "    train_loss = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    test_loss, group_loss = evaluate(\n",
        "        model,\n",
        "        eval_loader,\n",
        "        criterion,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    for k in group_losses_week:\n",
        "\n",
        "        if k not in group_loss:\n",
        "            group_losses_week[k].append(None)\n",
        "\n",
        "        else:\n",
        "            group_losses_week[k].append(group_loss[k])\n",
        "\n",
        "    group_losses.append(group_loss)\n",
        "\n",
        "    print(f\"[{epoch+1:02d}] train_loss={train_loss:.4f}  test_loss={test_loss:.4f}\")\n",
        "    print(group_loss)\n",
        "    print()"
      ],
      "id": "9b1aec55272f0c8b",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59177ad186a241319682a0521ddde247"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01] train_loss=15.9002  test_loss=15.4630\n",
            "{'29': 58.8474, '30': 49.3266, '31': 39.2382, '32': 38.1925, '33': 31.1415, '34': 22.4941, '35': 16.1783, '36': 9.5747, '37': 6.4531, '38': 6.4016, '39': 9.5019, '40': 12.4273, '41': 14.7275}\n",
            "\n",
            "[02] train_loss=14.9451  test_loss=14.4942\n",
            "{'29': 52.3154, '30': 42.3425, '31': 33.9036, '32': 33.1781, '33': 25.675, '34': 17.3135, '35': 11.896, '36': 6.8842, '37': 6.882, '38': 9.0435, '39': 12.4557, '40': 15.477, '41': 17.2251}\n",
            "\n",
            "[03] train_loss=14.1845  test_loss=12.9341\n",
            "{'29': 49.0595, '30': 37.6802, '31': 31.3202, '32': 30.9586, '33': 23.6678, '34': 16.4256, '35': 11.6829, '36': 7.6635, '37': 6.7817, '38': 7.4071, '39': 9.5063, '40': 11.3317, '41': 11.4522}\n",
            "\n",
            "[04] train_loss=12.4302  test_loss=10.0066\n",
            "{'29': 36.4158, '30': 22.1485, '31': 20.7085, '32': 21.8152, '33': 15.5645, '34': 10.7365, '35': 9.1087, '36': 7.902, '37': 8.0621, '38': 7.5863, '39': 7.3335, '40': 7.2373, '41': 5.9261}\n",
            "\n",
            "[05] train_loss=9.8375  test_loss=8.7095\n",
            "{'29': 28.9083, '30': 15.0536, '31': 15.6047, '32': 18.4774, '33': 14.1675, '34': 10.2356, '35': 10.3437, '36': 9.2645, '37': 7.8994, '38': 6.0379, '39': 4.6919, '40': 3.5079, '41': 3.5032}\n",
            "\n",
            "[06] train_loss=9.7994  test_loss=11.6247\n",
            "{'29': 29.4062, '30': 17.6223, '31': 19.955, '32': 23.0707, '33': 20.2985, '34': 16.6388, '35': 15.8942, '36': 14.2477, '37': 9.9726, '38': 6.9446, '39': 4.8388, '40': 3.2389, '41': 2.7259}\n",
            "\n",
            "[07] train_loss=9.1294  test_loss=12.3051\n",
            "{'29': 30.2577, '30': 18.9776, '31': 21.1456, '32': 24.3633, '33': 21.7353, '34': 18.5411, '35': 17.3898, '36': 15.1326, '37': 10.3198, '38': 6.8852, '39': 4.9129, '40': 3.1376, '41': 2.6058}\n",
            "\n",
            "[08] train_loss=9.7202  test_loss=12.3687\n",
            "{'29': 27.1494, '30': 16.4343, '31': 19.6274, '32': 22.7881, '33': 21.0961, '34': 17.7939, '35': 17.8765, '36': 15.6868, '37': 11.0, '38': 7.5132, '39': 5.196, '40': 3.2912, '41': 2.5434}\n",
            "\n",
            "[09] train_loss=8.9226  test_loss=12.1183\n",
            "{'29': 4.995, '30': 15.7868, '31': 15.4934, '32': 14.1054, '33': 13.3054, '34': 13.3242, '35': 12.7536, '36': 12.2139, '37': 13.7905, '38': 11.8548, '39': 10.8045, '40': 8.7578, '41': 4.9404}\n",
            "\n",
            "[10] train_loss=8.8665  test_loss=6.1683\n",
            "{'29': 8.7018, '30': 6.0384, '31': 6.9969, '32': 9.71, '33': 7.8813, '34': 4.8237, '35': 7.1923, '36': 6.6462, '37': 7.2035, '38': 5.864, '39': 5.0093, '40': 3.7075, '41': 2.9229}\n",
            "\n",
            "[11] train_loss=8.3102  test_loss=9.8434\n",
            "{'29': 4.9346, '30': 8.1517, '31': 9.5873, '32': 9.1416, '33': 8.7442, '34': 9.279, '35': 9.5971, '36': 9.8678, '37': 11.9508, '38': 10.9036, '39': 9.9937, '40': 8.7595, '41': 5.5044}\n",
            "\n",
            "[12] train_loss=8.4657  test_loss=6.5729\n",
            "{'29': 12.9374, '30': 6.7886, '31': 8.719, '32': 11.783, '33': 9.9772, '34': 5.969, '35': 7.5553, '36': 7.0758, '37': 7.1002, '38': 5.711, '39': 4.6025, '40': 3.4958, '41': 2.7383}\n",
            "\n",
            "[13] train_loss=8.4325  test_loss=8.7374\n",
            "{'29': 18.6753, '30': 9.7429, '31': 13.1464, '32': 16.3814, '33': 14.5949, '34': 10.6528, '35': 11.1068, '36': 10.4195, '37': 8.3119, '38': 6.2005, '39': 4.8342, '40': 3.3867, '41': 2.4608}\n",
            "\n",
            "[14] train_loss=9.0672  test_loss=6.2823\n",
            "{'29': 6.0559, '30': 6.4195, '31': 7.8758, '32': 8.7279, '33': 8.1185, '34': 5.6137, '35': 7.327, '36': 6.3705, '37': 7.7036, '38': 5.8774, '39': 4.9916, '40': 3.9074, '41': 2.711}\n",
            "\n",
            "[15] train_loss=8.3389  test_loss=10.8041\n",
            "{'29': 6.4077, '30': 7.3315, '31': 8.6875, '32': 8.7967, '33': 7.9984, '34': 8.1351, '35': 9.5899, '36': 9.9356, '37': 12.8499, '38': 13.5351, '39': 12.4042, '40': 11.4774, '41': 9.1746}\n",
            "\n",
            "[16] train_loss=7.2291  test_loss=6.0369\n",
            "{'29': 7.2991, '30': 6.3175, '31': 7.0251, '32': 9.3219, '33': 7.5944, '34': 5.098, '35': 7.3553, '36': 6.8438, '37': 6.7463, '38': 5.8195, '39': 4.6956, '40': 3.2468, '41': 2.5553}\n",
            "\n",
            "[17] train_loss=8.7385  test_loss=7.7320\n",
            "{'29': 5.3875, '30': 8.4321, '31': 9.5039, '32': 8.7886, '33': 7.8093, '34': 7.8522, '35': 8.12, '36': 7.9932, '37': 9.3785, '38': 7.8372, '39': 6.9329, '40': 5.5202, '41': 3.3315}\n",
            "\n",
            "[18] train_loss=8.8833  test_loss=8.3271\n",
            "{'29': 6.135, '30': 8.5614, '31': 9.448, '32': 9.0091, '33': 7.8453, '34': 7.7887, '35': 8.0761, '36': 8.3147, '37': 10.0968, '38': 8.7816, '39': 8.3502, '40': 6.7722, '41': 4.1234}\n",
            "\n",
            "[19] train_loss=7.9916  test_loss=6.9437\n",
            "{'29': 14.1276, '30': 7.0964, '31': 8.9567, '32': 11.8616, '33': 9.2817, '34': 7.258, '35': 8.3655, '36': 8.0541, '37': 7.1295, '38': 5.9644, '39': 4.9902, '40': 3.3928, '41': 2.5902}\n",
            "\n",
            "[20] train_loss=7.8754  test_loss=5.7855\n",
            "{'29': 10.2084, '30': 5.8903, '31': 6.9121, '32': 9.3293, '33': 7.2463, '34': 5.1236, '35': 5.9523, '36': 6.002, '37': 6.6093, '38': 5.7906, '39': 4.7518, '40': 3.4839, '41': 2.6686}\n",
            "\n",
            "[21] train_loss=6.9524  test_loss=6.8033\n",
            "{'29': 11.7774, '30': 5.793, '31': 8.2509, '32': 11.737, '33': 10.1017, '34': 7.1485, '35': 7.6078, '36': 7.933, '37': 7.2226, '38': 5.809, '39': 4.7699, '40': 3.4888, '41': 2.4833}\n",
            "\n",
            "[22] train_loss=6.5130  test_loss=7.9051\n",
            "{'29': 6.61, '30': 9.5405, '31': 10.0917, '32': 9.1434, '33': 7.8301, '34': 8.2813, '35': 8.0878, '36': 8.0849, '37': 9.2969, '38': 7.6336, '39': 7.5142, '40': 5.8718, '41': 3.438}\n",
            "\n",
            "[23] train_loss=6.9313  test_loss=5.4222\n",
            "{'29': 6.9705, '30': 6.489, '31': 6.4446, '32': 7.9077, '33': 6.5422, '34': 4.8889, '35': 5.8425, '36': 5.9993, '37': 6.2016, '38': 5.3065, '39': 4.3233, '40': 3.2763, '41': 2.7155}\n",
            "\n",
            "[24] train_loss=7.9798  test_loss=6.7481\n",
            "{'29': 11.7043, '30': 6.3231, '31': 8.1193, '32': 11.7596, '33': 9.6926, '34': 7.1372, '35': 7.8603, '36': 7.578, '37': 7.2405, '38': 5.8263, '39': 4.8102, '40': 3.1656, '41': 2.66}\n",
            "\n",
            "[25] train_loss=8.2468  test_loss=7.7044\n",
            "{'29': 13.98, '30': 7.1184, '31': 9.5775, '32': 13.2196, '33': 11.2526, '34': 8.3032, '35': 8.7833, '36': 8.7307, '37': 8.0032, '38': 6.6563, '39': 5.5783, '40': 3.7698, '41': 2.6665}\n",
            "\n",
            "[26] train_loss=6.6061  test_loss=5.7009\n",
            "{'29': 8.1279, '30': 6.5367, '31': 6.9434, '32': 7.6234, '33': 6.6314, '34': 5.7202, '35': 6.0322, '36': 5.9824, '37': 6.8487, '38': 5.5802, '39': 4.4485, '40': 3.5742, '41': 3.0694}\n",
            "\n",
            "[27] train_loss=6.5205  test_loss=6.6472\n",
            "{'29': 8.3028, '30': 5.6262, '31': 6.952, '32': 10.2835, '33': 8.9597, '34': 6.486, '35': 7.2944, '36': 7.8753, '37': 7.3506, '38': 6.1729, '39': 5.2883, '40': 3.5728, '41': 2.4977}\n",
            "\n",
            "[28] train_loss=6.6844  test_loss=8.7508\n",
            "{'29': 15.8223, '30': 9.0596, '31': 11.419, '32': 14.9158, '33': 13.3392, '34': 10.1692, '35': 11.1089, '36': 10.7636, '37': 8.838, '38': 6.8552, '39': 5.2171, '40': 3.7107, '41': 2.4088}\n",
            "\n",
            "[29] train_loss=8.3474  test_loss=8.2272\n",
            "{'29': 5.9951, '30': 8.5727, '31': 9.7056, '32': 8.522, '33': 7.8383, '34': 8.6478, '35': 8.2236, '36': 8.5488, '37': 9.8596, '38': 8.5188, '39': 7.5617, '40': 6.315, '41': 4.2843}\n",
            "\n",
            "[30] train_loss=5.9491  test_loss=5.4416\n",
            "{'29': 8.4564, '30': 5.5423, '31': 6.5505, '32': 9.4492, '33': 6.8495, '34': 5.1422, '35': 5.6908, '36': 5.8393, '37': 5.8548, '38': 5.0588, '39': 4.5267, '40': 3.3635, '41': 2.6997}\n",
            "\n",
            "[31] train_loss=6.0822  test_loss=5.4365\n",
            "{'29': 8.8305, '30': 5.7845, '31': 5.9369, '32': 8.5725, '33': 6.7381, '34': 5.1852, '35': 6.1245, '36': 6.0811, '37': 5.9263, '38': 5.2189, '39': 4.2737, '40': 3.0385, '41': 2.7802}\n",
            "\n",
            "[32] train_loss=8.4990  test_loss=8.2816\n",
            "{'29': 16.3362, '30': 8.8331, '31': 10.5396, '32': 14.2266, '33': 12.6774, '34': 9.7202, '35': 10.2979, '36': 9.9739, '37': 8.1765, '38': 6.6883, '39': 5.1178, '40': 3.4324, '41': 2.463}\n",
            "\n",
            "[33] train_loss=7.2670  test_loss=7.5952\n",
            "{'29': 8.5763, '30': 13.3219, '31': 13.3466, '32': 11.1469, '33': 9.4028, '34': 8.9365, '35': 8.0823, '36': 7.2905, '37': 8.2514, '38': 6.4528, '39': 6.0305, '40': 4.4394, '41': 3.0501}\n",
            "\n",
            "[34] train_loss=6.8016  test_loss=19.0479\n",
            "{'29': 15.1641, '30': 22.3415, '31': 23.0168, '32': 20.9864, '33': 19.694, '34': 20.7677, '35': 19.3705, '36': 19.3187, '37': 21.4521, '38': 19.453, '39': 17.2265, '40': 14.6096, '41': 8.951}\n",
            "\n",
            "[35] train_loss=8.9282  test_loss=8.1970\n",
            "{'29': 11.5885, '30': 6.9019, '31': 8.8738, '32': 13.2726, '33': 12.1206, '34': 9.7517, '35': 10.5181, '36': 10.8835, '37': 8.4116, '38': 6.4084, '39': 5.1357, '40': 3.2164, '41': 2.5225}\n",
            "\n",
            "[36] train_loss=6.0789  test_loss=5.2727\n",
            "{'29': 5.8087, '30': 4.8144, '31': 6.3917, '32': 8.1642, '33': 5.8113, '34': 4.8059, '35': 5.3505, '36': 5.6337, '37': 6.2945, '38': 4.8354, '39': 4.6977, '40': 3.7579, '41': 3.0319}\n",
            "\n",
            "[37] train_loss=5.7271  test_loss=7.5545\n",
            "{'29': 11.7053, '30': 6.5286, '31': 8.8516, '32': 12.5127, '33': 10.5812, '34': 8.121, '35': 8.5865, '36': 9.2719, '37': 7.8112, '38': 6.7617, '39': 5.4911, '40': 3.4582, '41': 2.3923}\n",
            "\n",
            "[38] train_loss=8.7280  test_loss=9.7763\n",
            "{'29': 6.3516, '30': 8.9478, '31': 10.6227, '32': 9.0202, '33': 8.0837, '34': 9.3187, '35': 9.2794, '36': 9.7903, '37': 12.0907, '38': 10.7698, '39': 10.0929, '40': 8.3251, '41': 5.0349}\n",
            "\n",
            "[39] train_loss=7.0425  test_loss=6.5528\n",
            "{'29': 6.1692, '30': 7.1859, '31': 8.6914, '32': 7.6801, '33': 6.4016, '34': 6.7928, '35': 6.8371, '36': 7.1621, '37': 8.084, '38': 6.2705, '39': 5.6206, '40': 4.3653, '41': 3.3885}\n",
            "\n",
            "[40] train_loss=6.6827  test_loss=6.4294\n",
            "{'29': 7.0243, '30': 5.1577, '31': 6.4835, '32': 9.9097, '33': 8.525, '34': 6.5032, '35': 7.3453, '36': 7.9608, '37': 7.0838, '38': 5.9704, '39': 4.8647, '40': 3.1678, '41': 2.5065}\n",
            "\n",
            "[41] train_loss=5.5406  test_loss=5.2220\n",
            "{'29': 6.8037, '30': 4.5935, '31': 6.5728, '32': 9.2594, '33': 6.5089, '34': 5.0809, '35': 5.6036, '36': 5.5871, '37': 5.6057, '38': 4.9246, '39': 4.3527, '40': 2.9475, '41': 2.5975}\n",
            "\n",
            "[42] train_loss=6.2055  test_loss=5.7408\n",
            "{'29': 9.213, '30': 5.381, '31': 6.4496, '32': 9.6853, '33': 7.5239, '34': 5.945, '35': 6.23, '36': 6.3341, '37': 6.0822, '38': 5.4946, '39': 4.5054, '40': 2.9917, '41': 2.5291}\n",
            "\n",
            "[43] train_loss=7.2083  test_loss=5.1854\n",
            "{'29': 6.1302, '30': 5.6042, '31': 7.3721, '32': 7.7796, '33': 5.488, '34': 4.9993, '35': 5.3328, '36': 5.5407, '37': 6.1794, '38': 4.8445, '39': 4.4754, '40': 3.2205, '41': 2.7415}\n",
            "\n",
            "[44] train_loss=5.4911  test_loss=5.1151\n",
            "{'29': 5.9657, '30': 5.0271, '31': 6.1528, '32': 7.7758, '33': 6.3285, '34': 4.7356, '35': 4.8718, '36': 5.7012, '37': 5.7902, '38': 5.1267, '39': 4.425, '40': 3.0551, '41': 2.5709}\n",
            "\n",
            "[45] train_loss=5.9368  test_loss=5.4045\n",
            "{'29': 8.6272, '30': 4.9303, '31': 6.462, '32': 9.2026, '33': 6.4498, '34': 5.2713, '35': 5.6318, '36': 5.6432, '37': 5.6918, '38': 5.3277, '39': 4.6398, '40': 3.3404, '41': 2.769}\n",
            "\n",
            "[46] train_loss=5.8882  test_loss=4.8995\n",
            "{'29': 6.1089, '30': 4.8084, '31': 6.9923, '32': 7.9298, '33': 5.1558, '34': 4.6418, '35': 5.0103, '36': 5.3779, '37': 5.5397, '38': 4.823, '39': 4.1352, '40': 2.8618, '41': 2.792}\n",
            "\n",
            "[47] train_loss=6.4913  test_loss=5.4682\n",
            "{'29': 5.8835, '30': 5.6382, '31': 7.8564, '32': 8.3555, '33': 5.7297, '34': 5.3709, '35': 5.7429, '36': 5.9244, '37': 6.386, '38': 5.1053, '39': 4.7213, '40': 3.2754, '41': 2.8755}\n",
            "\n",
            "[48] train_loss=5.4543  test_loss=5.5621\n",
            "{'29': 7.2149, '30': 5.7939, '31': 7.8068, '32': 7.734, '33': 5.5017, '34': 5.7272, '35': 5.6302, '36': 6.114, '37': 6.5039, '38': 5.5159, '39': 4.5924, '40': 3.4428, '41': 3.1832}\n",
            "\n",
            "[49] train_loss=6.8267  test_loss=10.5206\n",
            "{'29': 11.0441, '30': 15.2188, '31': 16.228, '32': 13.7769, '33': 12.1487, '34': 12.8585, '35': 11.4273, '36': 10.7799, '37': 11.6464, '38': 9.3356, '39': 8.142, '40': 6.6066, '41': 4.262}\n",
            "\n",
            "[50] train_loss=6.1553  test_loss=7.8217\n",
            "{'29': 8.106, '30': 5.2733, '31': 7.1541, '32': 11.1678, '33': 10.1408, '34': 8.0726, '35': 9.3502, '36': 9.9828, '37': 8.7381, '38': 7.6059, '39': 5.7277, '40': 3.6184, '41': 2.3052}\n",
            "\n",
            "[51] train_loss=6.9621  test_loss=5.0450\n",
            "{'29': 7.888, '30': 4.9724, '31': 5.7241, '32': 8.4713, '33': 6.5814, '34': 4.8461, '35': 5.1704, '36': 5.4864, '37': 5.5303, '38': 4.8409, '39': 4.0211, '40': 2.9547, '41': 2.9252}\n",
            "\n",
            "[52] train_loss=6.3237  test_loss=8.3480\n",
            "{'29': 7.5835, '30': 8.3193, '31': 10.172, '32': 8.4704, '33': 7.7603, '34': 8.6677, '35': 8.2299, '36': 8.5509, '37': 10.2153, '38': 8.6107, '39': 7.7785, '40': 6.3813, '41': 4.4562}\n",
            "\n",
            "[53] train_loss=6.1990  test_loss=4.9442\n",
            "{'29': 6.1366, '30': 4.4436, '31': 6.2376, '32': 8.1926, '33': 6.1558, '34': 4.5635, '35': 4.6887, '36': 5.2564, '37': 5.6167, '38': 4.9968, '39': 4.1808, '40': 2.9696, '41': 2.6604}\n",
            "\n",
            "[54] train_loss=5.6796  test_loss=4.8766\n",
            "{'29': 7.0765, '30': 5.2288, '31': 6.7608, '32': 7.2255, '33': 5.1372, '34': 4.6637, '35': 4.9082, '36': 5.3118, '37': 5.8455, '38': 4.5775, '39': 4.1187, '40': 3.0617, '41': 2.764}\n",
            "\n",
            "[55] train_loss=5.9085  test_loss=5.1569\n",
            "{'29': 6.1272, '30': 4.2652, '31': 6.3992, '32': 9.1705, '33': 6.1005, '34': 4.783, '35': 5.1374, '36': 5.274, '37': 5.7661, '38': 5.1787, '39': 4.5307, '40': 3.1092, '41': 2.5966}\n",
            "\n",
            "[56] train_loss=6.5590  test_loss=5.2191\n",
            "{'29': 6.2691, '30': 5.016, '31': 7.058, '32': 8.6462, '33': 5.3005, '34': 4.7419, '35': 5.6084, '36': 5.7981, '37': 5.9143, '38': 5.0577, '39': 4.6455, '40': 2.9152, '41': 2.4175}\n",
            "\n",
            "[57] train_loss=5.5665  test_loss=5.3011\n",
            "{'29': 6.9607, '30': 4.6361, '31': 6.1356, '32': 8.2878, '33': 5.6582, '34': 4.9857, '35': 5.7197, '36': 6.035, '37': 5.9213, '38': 5.35, '39': 4.5952, '40': 2.9398, '41': 2.4102}\n",
            "\n",
            "[58] train_loss=6.5202  test_loss=6.2776\n",
            "{'29': 7.1742, '30': 7.3666, '31': 9.1693, '32': 7.4577, '33': 6.4572, '34': 6.5146, '35': 6.2726, '36': 6.4674, '37': 7.2502, '38': 5.901, '39': 5.6268, '40': 4.6706, '41': 3.7955}\n",
            "\n",
            "[59] train_loss=8.2955  test_loss=8.9421\n",
            "{'29': 14.3154, '30': 10.1579, '31': 11.2743, '32': 14.9071, '33': 13.5422, '34': 11.2267, '35': 11.4623, '36': 10.9977, '37': 8.6475, '38': 7.0326, '39': 5.505, '40': 3.4822, '41': 2.4458}\n",
            "\n",
            "[60] train_loss=7.9659  test_loss=10.4923\n",
            "{'29': 8.7555, '30': 10.2181, '31': 12.6551, '32': 10.4851, '33': 10.0029, '34': 11.1031, '35': 10.4894, '36': 10.8152, '37': 12.4388, '38': 10.6097, '39': 9.9093, '40': 8.2983, '41': 5.7303}\n",
            "\n",
            "[61] train_loss=5.7500  test_loss=4.7481\n",
            "{'29': 6.1393, '30': 4.2745, '31': 6.5908, '32': 7.1892, '33': 5.2582, '34': 4.5557, '35': 4.6126, '36': 5.2062, '37': 5.4589, '38': 4.6427, '39': 4.0042, '40': 3.0311, '41': 3.0196}\n",
            "\n",
            "[62] train_loss=5.7251  test_loss=5.8256\n",
            "{'29': 7.7566, '30': 7.1843, '31': 9.2847, '32': 7.6132, '33': 6.4652, '34': 6.6775, '35': 5.9427, '36': 6.271, '37': 6.7901, '38': 5.1701, '39': 4.5509, '40': 3.6152, '41': 3.1482}\n",
            "\n",
            "[63] train_loss=6.9009  test_loss=4.7611\n",
            "{'29': 5.5049, '30': 4.3161, '31': 6.4001, '32': 7.6971, '33': 5.2417, '34': 4.4197, '35': 5.0079, '36': 5.0897, '37': 5.4244, '38': 4.595, '39': 4.1592, '40': 2.8962, '41': 2.5885}\n",
            "\n",
            "[64] train_loss=4.8192  test_loss=4.7766\n",
            "{'29': 6.6162, '30': 5.3523, '31': 7.3116, '32': 7.0644, '33': 4.9422, '34': 4.7921, '35': 4.715, '36': 5.3504, '37': 5.6082, '38': 4.5042, '39': 3.8532, '40': 2.911, '41': 2.7443}\n",
            "\n",
            "[65] train_loss=5.2070  test_loss=5.1806\n",
            "{'29': 6.7527, '30': 4.9739, '31': 6.1832, '32': 9.1587, '33': 6.7878, '34': 5.1919, '35': 5.5709, '36': 5.3696, '37': 5.579, '38': 4.7315, '39': 4.2494, '40': 3.1616, '41': 2.4661}\n",
            "\n",
            "[66] train_loss=5.4255  test_loss=5.4853\n",
            "{'29': 8.6457, '30': 7.1157, '31': 9.0463, '32': 7.5596, '33': 5.6777, '34': 5.8706, '35': 5.1737, '36': 5.8702, '37': 6.2342, '38': 4.9952, '39': 4.5668, '40': 3.7642, '41': 3.2252}\n",
            "\n",
            "[67] train_loss=5.5871  test_loss=6.1614\n",
            "{'29': 7.7928, '30': 5.1363, '31': 6.8606, '32': 9.9828, '33': 7.9411, '34': 6.2311, '35': 6.6421, '36': 6.879, '37': 6.3555, '38': 6.111, '39': 5.1126, '40': 3.4827, '41': 2.5283}\n",
            "\n",
            "[68] train_loss=5.9870  test_loss=5.6542\n",
            "{'29': 10.4697, '30': 9.7347, '31': 11.594, '32': 9.1812, '33': 6.447, '34': 6.1208, '35': 5.2849, '36': 5.7127, '37': 6.0712, '38': 4.7324, '39': 4.5887, '40': 3.4514, '41': 2.6596}\n",
            "\n",
            "[69] train_loss=6.4118  test_loss=7.2372\n",
            "{'29': 10.8758, '30': 7.1661, '31': 8.295, '32': 12.28, '33': 10.8101, '34': 8.3297, '35': 8.7468, '36': 8.4661, '37': 7.0723, '38': 6.1091, '39': 5.1143, '40': 3.4752, '41': 2.3233}\n",
            "\n",
            "[70] train_loss=5.9556  test_loss=5.4822\n",
            "{'29': 5.865, '30': 4.1239, '31': 6.091, '32': 8.3337, '33': 5.9786, '34': 4.9537, '35': 5.6978, '36': 6.3492, '37': 6.0545, '38': 5.7176, '39': 4.9269, '40': 3.2718, '41': 2.2808}\n",
            "\n",
            "[71] train_loss=5.8233  test_loss=4.5750\n",
            "{'29': 5.9247, '30': 3.9951, '31': 6.2178, '32': 7.2878, '33': 4.9717, '34': 4.3425, '35': 4.6792, '36': 4.9917, '37': 5.1679, '38': 4.5066, '39': 3.8765, '40': 2.8434, '41': 2.5823}\n",
            "\n",
            "[72] train_loss=6.0398  test_loss=5.0396\n",
            "{'29': 6.4635, '30': 5.2748, '31': 7.6732, '32': 7.2076, '33': 5.1017, '34': 5.0997, '35': 4.9025, '36': 5.5148, '37': 6.0677, '38': 4.6182, '39': 4.1238, '40': 3.4526, '41': 3.1326}\n",
            "\n",
            "[73] train_loss=6.3543  test_loss=6.9536\n",
            "{'29': 8.6883, '30': 10.1429, '31': 11.7343, '32': 9.4258, '33': 8.2954, '34': 8.3913, '35': 7.3261, '36': 7.0714, '37': 7.9171, '38': 5.8653, '39': 5.1774, '40': 4.3229, '41': 3.3008}\n",
            "\n",
            "[74] train_loss=5.7446  test_loss=6.6127\n",
            "{'29': 5.5917, '30': 4.5465, '31': 7.1569, '32': 7.3484, '33': 5.7676, '34': 6.0444, '35': 6.2262, '36': 6.76, '37': 8.0733, '38': 6.682, '39': 6.8388, '40': 6.0205, '41': 4.3727}\n",
            "\n",
            "[75] train_loss=6.3567  test_loss=5.6190\n",
            "{'29': 6.2625, '30': 4.5568, '31': 6.1669, '32': 8.6543, '33': 6.1773, '34': 5.181, '35': 6.0023, '36': 6.5288, '37': 6.1626, '38': 5.9055, '39': 4.8433, '40': 3.1924, '41': 2.3073}\n",
            "\n",
            "[76] train_loss=5.2292  test_loss=5.2331\n",
            "{'29': 6.9581, '30': 5.6456, '31': 7.8047, '32': 7.181, '33': 5.7312, '34': 5.6219, '35': 5.1789, '36': 5.7548, '37': 6.2234, '38': 4.6681, '39': 4.1079, '40': 3.554, '41': 3.037}\n",
            "\n",
            "[77] train_loss=5.5335  test_loss=4.9685\n",
            "{'29': 6.6574, '30': 4.9613, '31': 7.4557, '32': 7.8827, '33': 4.8133, '34': 4.8332, '35': 4.8925, '36': 5.1702, '37': 5.9918, '38': 4.7902, '39': 4.2763, '40': 3.1692, '41': 2.699}\n",
            "\n",
            "[78] train_loss=6.1128  test_loss=4.9522\n",
            "{'29': 9.2763, '30': 4.6421, '31': 5.745, '32': 8.1884, '33': 5.9165, '34': 5.0742, '35': 5.2069, '36': 5.3257, '37': 5.3304, '38': 4.6997, '39': 4.0894, '40': 3.001, '41': 2.5555}\n",
            "\n",
            "[79] train_loss=5.4096  test_loss=4.6532\n",
            "{'29': 7.1679, '30': 3.7462, '31': 6.0794, '32': 7.7263, '33': 5.2502, '34': 4.6618, '35': 4.4358, '36': 4.8892, '37': 5.4816, '38': 4.4113, '39': 3.9059, '40': 2.9955, '41': 2.7658}\n",
            "\n",
            "[80] train_loss=5.1866  test_loss=4.7280\n",
            "{'29': 6.4388, '30': 3.8532, '31': 6.5976, '32': 7.9936, '33': 5.0334, '34': 4.5674, '35': 4.6514, '36': 4.9687, '37': 5.5578, '38': 4.5535, '39': 3.9945, '40': 2.9996, '41': 2.7279}\n",
            "\n",
            "[81] train_loss=5.0675  test_loss=5.0713\n",
            "{'29': 8.6506, '30': 7.1586, '31': 8.5838, '32': 7.424, '33': 5.9362, '34': 5.7809, '35': 4.9325, '36': 5.4405, '37': 5.76, '38': 4.4678, '39': 3.8001, '40': 2.9067, '41': 2.7175}\n",
            "\n",
            "[82] train_loss=5.0244  test_loss=5.1326\n",
            "{'29': 7.1587, '30': 4.1482, '31': 5.7057, '32': 8.2505, '33': 6.6596, '34': 5.1339, '35': 5.4398, '36': 5.7049, '37': 5.4891, '38': 4.9271, '39': 4.2295, '40': 3.0252, '41': 2.5017}\n",
            "\n",
            "[83] train_loss=5.3050  test_loss=5.3499\n",
            "{'29': 7.8124, '30': 6.0535, '31': 8.201, '32': 7.5132, '33': 5.6179, '34': 5.6964, '35': 5.2883, '36': 5.7745, '37': 6.5075, '38': 4.7652, '39': 4.3498, '40': 3.3859, '41': 2.9476}\n",
            "\n",
            "[84] train_loss=5.2616  test_loss=5.3694\n",
            "{'29': 8.2173, '30': 6.1911, '31': 8.4815, '32': 7.6855, '33': 5.7324, '34': 5.9882, '35': 5.2718, '36': 5.852, '37': 6.3512, '38': 4.7156, '39': 4.2824, '40': 3.3111, '41': 3.0151}\n",
            "\n",
            "[85] train_loss=5.0844  test_loss=5.6428\n",
            "{'29': 8.5221, '30': 4.6878, '31': 5.8459, '32': 8.096, '33': 6.9248, '34': 5.8325, '35': 6.3525, '36': 6.2829, '37': 6.0998, '38': 5.5535, '39': 4.5896, '40': 3.3958, '41': 2.3408}\n",
            "\n",
            "[86] train_loss=5.2916  test_loss=6.5789\n",
            "{'29': 9.4301, '30': 8.1536, '31': 10.1959, '32': 8.165, '33': 7.3678, '34': 7.802, '35': 6.8312, '36': 7.0691, '37': 7.7388, '38': 5.4342, '39': 5.1267, '40': 4.477, '41': 3.3954}\n",
            "\n",
            "[87] train_loss=5.4143  test_loss=8.1155\n",
            "{'29': 7.1453, '30': 6.0015, '31': 7.6368, '32': 11.4142, '33': 10.2199, '34': 8.5528, '35': 9.9944, '36': 10.4351, '37': 8.9534, '38': 7.9639, '39': 5.7262, '40': 3.7908, '41': 2.1978}\n",
            "\n",
            "[88] train_loss=5.9215  test_loss=4.8959\n",
            "{'29': 7.8818, '30': 4.1963, '31': 5.9997, '32': 7.5527, '33': 5.0124, '34': 4.6238, '35': 4.4746, '36': 5.1619, '37': 5.708, '38': 4.5209, '39': 4.4639, '40': 3.9449, '41': 3.4827}\n",
            "\n",
            "[89] train_loss=5.9839  test_loss=5.9948\n",
            "{'29': 8.552, '30': 7.0488, '31': 9.0574, '32': 7.7131, '33': 6.2775, '34': 6.6455, '35': 6.0579, '36': 6.4247, '37': 7.2903, '38': 5.2343, '39': 4.8596, '40': 3.9662, '41': 3.3}\n",
            "\n",
            "[90] train_loss=5.2869  test_loss=5.2952\n",
            "{'29': 6.5527, '30': 3.4693, '31': 5.4048, '32': 8.1732, '33': 6.4573, '34': 5.1242, '35': 5.4161, '36': 6.0798, '37': 5.7188, '38': 5.4581, '39': 4.6208, '40': 3.1221, '41': 2.2591}\n",
            "\n",
            "[91] train_loss=4.9412  test_loss=5.2800\n",
            "{'29': 7.0815, '30': 3.6771, '31': 5.5432, '32': 7.5322, '33': 5.6524, '34': 4.8991, '35': 5.1774, '36': 5.9007, '37': 5.7113, '38': 5.8548, '39': 4.881, '40': 3.4599, '41': 2.322}\n",
            "\n",
            "[92] train_loss=5.6008  test_loss=4.8118\n",
            "{'29': 6.6179, '30': 4.3812, '31': 6.7937, '32': 7.0981, '33': 5.0995, '34': 5.0245, '35': 4.6522, '36': 5.1254, '37': 5.7393, '38': 4.4343, '39': 3.8862, '40': 3.4887, '41': 2.9675}\n",
            "\n",
            "[93] train_loss=4.9328  test_loss=4.6811\n",
            "{'29': 7.1435, '30': 3.8329, '31': 6.3614, '32': 7.4571, '33': 4.7631, '34': 4.5416, '35': 4.534, '36': 5.0111, '37': 5.3612, '38': 4.4866, '39': 4.0257, '40': 3.3137, '41': 3.1157}\n",
            "\n",
            "[94] train_loss=5.0776  test_loss=6.8585\n",
            "{'29': 9.7338, '30': 9.1126, '31': 10.9706, '32': 8.9063, '33': 6.9196, '34': 7.4507, '35': 6.8434, '36': 6.9053, '37': 7.988, '38': 6.3046, '39': 5.9249, '40': 4.6881, '41': 3.5507}\n",
            "\n",
            "[95] train_loss=5.3266  test_loss=4.6218\n",
            "{'29': 6.3536, '30': 3.4799, '31': 5.8363, '32': 7.7337, '33': 5.1054, '34': 4.4397, '35': 4.4472, '36': 4.877, '37': 5.339, '38': 4.4752, '39': 3.9343, '40': 3.1456, '41': 3.0315}\n",
            "\n",
            "[96] train_loss=5.3784  test_loss=9.0745\n",
            "{'29': 10.1458, '30': 9.0701, '31': 11.643, '32': 9.4777, '33': 8.5795, '34': 9.7806, '35': 9.208, '36': 9.371, '37': 10.5281, '38': 8.6464, '39': 8.6139, '40': 7.3437, '41': 5.1421}\n",
            "\n",
            "[97] train_loss=6.0614  test_loss=5.2834\n",
            "{'29': 6.8587, '30': 4.3217, '31': 6.7484, '32': 7.5698, '33': 5.1467, '34': 5.3281, '35': 5.1273, '36': 5.8113, '37': 6.3282, '38': 5.0656, '39': 4.5671, '40': 3.6792, '41': 3.4355}\n",
            "\n",
            "[98] train_loss=5.1105  test_loss=4.8218\n",
            "{'29': 6.3911, '30': 3.4125, '31': 5.839, '32': 7.9236, '33': 5.5221, '34': 4.8028, '35': 4.9778, '36': 5.321, '37': 5.2075, '38': 4.7228, '39': 4.2006, '40': 2.9607, '41': 2.4418}\n",
            "\n",
            "[99] train_loss=4.6463  test_loss=6.2436\n",
            "{'29': 10.3056, '30': 9.3201, '31': 10.767, '32': 8.4016, '33': 7.0484, '34': 7.398, '35': 5.8965, '36': 6.3387, '37': 7.0072, '38': 5.5852, '39': 4.8093, '40': 4.0949, '41': 3.449}\n",
            "\n",
            "[100] train_loss=6.0549  test_loss=4.8651\n",
            "{'29': 9.0816, '30': 6.3461, '31': 8.5685, '32': 7.5239, '33': 5.2804, '34': 5.459, '35': 4.5946, '36': 5.2508, '37': 5.4328, '38': 4.248, '39': 3.7873, '40': 3.0926, '41': 2.5628}\n",
            "\n",
            "[101] train_loss=5.8390  test_loss=4.5797\n",
            "{'29': 6.8342, '30': 3.4211, '31': 5.5634, '32': 7.8739, '33': 5.5946, '34': 4.6898, '35': 4.5084, '36': 4.8912, '37': 5.0526, '38': 4.3111, '39': 3.8065, '40': 2.9785, '41': 2.5535}\n",
            "\n",
            "[102] train_loss=5.7757  test_loss=7.3560\n",
            "{'29': 9.7324, '30': 8.1666, '31': 8.9132, '32': 12.5318, '33': 10.6896, '34': 8.1008, '35': 8.6147, '36': 8.5609, '37': 7.2653, '38': 6.604, '39': 5.2203, '40': 3.4508, '41': 2.2332}\n",
            "\n",
            "[103] train_loss=5.8302  test_loss=4.5504\n",
            "{'29': 6.6704, '30': 3.4454, '31': 6.2771, '32': 7.3912, '33': 4.9991, '34': 4.4329, '35': 4.4598, '36': 4.737, '37': 5.3859, '38': 4.3422, '39': 3.8696, '40': 2.9541, '41': 2.7803}\n",
            "\n",
            "[104] train_loss=5.5327  test_loss=5.5148\n",
            "{'29': 9.9718, '30': 8.3241, '31': 10.0894, '32': 8.6221, '33': 6.0254, '34': 6.2926, '35': 5.5361, '36': 5.7892, '37': 6.2356, '38': 4.5846, '39': 4.2079, '40': 3.1964, '41': 2.8972}\n",
            "\n",
            "[105] train_loss=4.8159  test_loss=5.3350\n",
            "{'29': 6.9747, '30': 4.0338, '31': 6.1158, '32': 8.7186, '33': 6.5796, '34': 5.1641, '35': 5.4268, '36': 5.6877, '37': 5.6016, '38': 5.4463, '39': 4.7793, '40': 3.2867, '41': 2.2982}\n",
            "\n",
            "[106] train_loss=5.6837  test_loss=4.9529\n",
            "{'29': 5.8219, '30': 3.5496, '31': 5.9021, '32': 8.6544, '33': 6.7331, '34': 5.149, '35': 4.9534, '36': 5.4286, '37': 5.1141, '38': 4.6388, '39': 4.1075, '40': 3.1046, '41': 2.4163}\n",
            "\n",
            "[107] train_loss=6.1610  test_loss=7.1377\n",
            "{'29': 7.9975, '30': 5.6967, '31': 8.6165, '32': 7.4712, '33': 6.5103, '34': 7.215, '35': 6.7849, '36': 7.4646, '37': 8.6278, '38': 6.8057, '39': 7.0346, '40': 6.2628, '41': 4.5119}\n",
            "\n",
            "[108] train_loss=6.4202  test_loss=4.8326\n",
            "{'29': 9.1407, '30': 6.9281, '31': 8.6673, '32': 7.4464, '33': 5.278, '34': 5.0873, '35': 4.5737, '36': 4.936, '37': 5.2143, '38': 4.5466, '39': 4.0631, '40': 2.8674, '41': 2.4893}\n",
            "\n",
            "[109] train_loss=4.8242  test_loss=4.5462\n",
            "{'29': 7.4071, '30': 4.0866, '31': 5.6268, '32': 7.8272, '33': 5.7659, '34': 4.572, '35': 4.3329, '36': 4.726, '37': 5.0432, '38': 4.3075, '39': 3.7078, '40': 2.9799, '41': 2.6959}\n",
            "\n",
            "[110] train_loss=5.8732  test_loss=4.9951\n",
            "{'29': 7.348, '30': 4.5366, '31': 5.8177, '32': 8.7894, '33': 6.895, '34': 5.2228, '35': 5.0683, '36': 5.4221, '37': 5.1923, '38': 4.6091, '39': 3.9585, '40': 3.023, '41': 2.5049}\n",
            "\n",
            "[111] train_loss=5.9329  test_loss=4.5002\n",
            "{'29': 6.6196, '30': 3.5289, '31': 5.9701, '32': 7.0351, '33': 5.1213, '34': 4.6377, '35': 4.2202, '36': 4.7211, '37': 5.1533, '38': 4.3056, '39': 3.7902, '40': 3.2146, '41': 2.6919}\n",
            "\n",
            "[112] train_loss=6.3372  test_loss=4.8611\n",
            "{'29': 6.2248, '30': 4.0784, '31': 5.4898, '32': 7.5235, '33': 6.119, '34': 4.7397, '35': 5.032, '36': 5.3565, '37': 5.2822, '38': 4.8535, '39': 4.0539, '40': 2.9791, '41': 2.4032}\n",
            "\n",
            "[113] train_loss=5.0921  test_loss=6.1328\n",
            "{'29': 7.5609, '30': 4.5213, '31': 6.1561, '32': 9.1133, '33': 7.6402, '34': 6.1595, '35': 7.0077, '36': 7.3593, '37': 6.5874, '38': 5.9883, '39': 4.9128, '40': 3.4108, '41': 2.2325}\n",
            "\n",
            "[114] train_loss=6.5328  test_loss=6.3385\n",
            "{'29': 8.9637, '30': 8.2331, '31': 10.0356, '32': 8.3095, '33': 6.7265, '34': 7.3742, '35': 6.5092, '36': 6.7657, '37': 7.4643, '38': 5.6224, '39': 4.9487, '40': 3.8642, '41': 3.282}\n",
            "\n",
            "[115] train_loss=5.6387  test_loss=5.1500\n",
            "{'29': 8.5836, '30': 6.0773, '31': 8.3206, '32': 7.5417, '33': 5.3469, '34': 5.7018, '35': 4.8407, '36': 5.4394, '37': 5.9117, '38': 4.7361, '39': 4.1785, '40': 3.397, '41': 2.9762}\n",
            "\n",
            "[116] train_loss=5.0532  test_loss=6.1813\n",
            "{'29': 8.3862, '30': 6.1144, '31': 7.1355, '32': 10.4881, '33': 8.7074, '34': 6.6886, '35': 6.999, '36': 6.8446, '37': 6.2035, '38': 5.6296, '39': 4.7138, '40': 3.4252, '41': 2.2803}\n",
            "\n",
            "[117] train_loss=6.3315  test_loss=4.9406\n",
            "{'29': 7.0102, '30': 3.7484, '31': 5.7745, '32': 8.4426, '33': 6.4924, '34': 5.1618, '35': 5.0027, '36': 5.3673, '37': 5.2415, '38': 4.5176, '39': 4.1842, '40': 3.0627, '41': 2.4635}\n",
            "\n",
            "[118] train_loss=5.8628  test_loss=4.4525\n",
            "{'29': 6.4139, '30': 3.0609, '31': 5.6909, '32': 7.2027, '33': 5.0565, '34': 4.5152, '35': 4.2427, '36': 4.6694, '37': 5.1808, '38': 4.1709, '39': 3.7985, '40': 3.1303, '41': 2.8258}\n",
            "\n",
            "[119] train_loss=5.0467  test_loss=6.0886\n",
            "{'29': 10.5445, '30': 9.8709, '31': 10.878, '32': 8.7647, '33': 6.8316, '34': 7.1605, '35': 6.0535, '36': 6.3074, '37': 7.1405, '38': 5.2427, '39': 4.3239, '40': 3.4592, '41': 3.0166}\n",
            "\n",
            "[120] train_loss=4.8434  test_loss=4.6570\n",
            "{'29': 7.0292, '30': 3.9014, '31': 5.4825, '32': 8.0775, '33': 5.8118, '34': 4.7743, '35': 4.7677, '36': 4.9548, '37': 5.2045, '38': 4.2875, '39': 3.7469, '40': 2.8532, '41': 2.7312}\n",
            "\n",
            "[121] train_loss=4.7779  test_loss=5.0248\n",
            "{'29': 6.9942, '30': 3.1397, '31': 5.3853, '32': 7.6187, '33': 5.7048, '34': 4.8349, '35': 4.5505, '36': 5.5812, '37': 5.2786, '38': 5.3591, '39': 4.746, '40': 3.578, '41': 2.3738}\n",
            "\n",
            "[122] train_loss=4.7232  test_loss=5.1510\n",
            "{'29': 7.211, '30': 4.1688, '31': 5.9402, '32': 8.4893, '33': 6.6262, '34': 5.3352, '35': 5.1987, '36': 5.7357, '37': 5.3026, '38': 4.9261, '39': 4.3741, '40': 3.2286, '41': 2.3525}\n",
            "\n",
            "[123] train_loss=5.0352  test_loss=4.5509\n",
            "{'29': 6.7635, '30': 3.4537, '31': 6.2675, '32': 6.8517, '33': 5.0396, '34': 4.8879, '35': 4.1312, '36': 4.8119, '37': 5.2911, '38': 4.2811, '39': 3.7973, '40': 3.3023, '41': 3.003}\n",
            "\n",
            "[124] train_loss=4.7995  test_loss=4.6351\n",
            "{'29': 7.6717, '30': 3.4269, '31': 5.689, '32': 7.6673, '33': 5.3249, '34': 4.7507, '35': 4.5601, '36': 4.9755, '37': 5.0936, '38': 4.5772, '39': 3.9184, '40': 2.8242, '41': 2.7998}\n",
            "\n",
            "[125] train_loss=5.1576  test_loss=6.3139\n",
            "{'29': 10.466, '30': 8.7551, '31': 10.7841, '32': 9.0047, '33': 6.9564, '34': 7.2966, '35': 6.3425, '36': 6.6625, '37': 7.3673, '38': 5.1655, '39': 4.9453, '40': 3.9036, '41': 3.3944}\n",
            "\n",
            "[126] train_loss=5.7831  test_loss=7.9015\n",
            "{'29': 8.6364, '30': 7.2181, '31': 8.0439, '32': 11.8036, '33': 10.1583, '34': 8.3459, '35': 9.6266, '36': 10.0128, '37': 8.7009, '38': 7.6618, '39': 5.3734, '40': 3.3534, '41': 2.2371}\n",
            "\n",
            "[127] train_loss=5.0980  test_loss=4.6609\n",
            "{'29': 6.6403, '30': 3.3581, '31': 6.3161, '32': 7.6977, '33': 4.7607, '34': 4.6228, '35': 4.5739, '36': 4.9618, '37': 5.4941, '38': 4.3586, '39': 4.1674, '40': 3.0199, '41': 2.669}\n",
            "\n",
            "[128] train_loss=6.0596  test_loss=4.9688\n",
            "{'29': 7.2334, '30': 4.0703, '31': 6.4461, '32': 6.6646, '33': 4.998, '34': 5.1948, '35': 4.3999, '36': 5.3628, '37': 5.8987, '38': 4.9445, '39': 4.26, '40': 3.6945, '41': 3.3587}\n",
            "\n",
            "[129] train_loss=5.1746  test_loss=6.0728\n",
            "{'29': 9.195, '30': 7.7651, '31': 9.7706, '32': 8.4275, '33': 6.6214, '34': 6.8889, '35': 6.0355, '36': 6.471, '37': 7.222, '38': 5.204, '39': 4.766, '40': 3.7989, '41': 3.3247}\n",
            "\n",
            "[130] train_loss=4.9943  test_loss=5.3318\n",
            "{'29': 8.8498, '30': 6.7492, '31': 7.2109, '32': 10.3936, '33': 8.1195, '34': 6.1028, '35': 5.5653, '36': 5.377, '37': 5.2191, '38': 4.369, '39': 3.9669, '40': 3.0234, '41': 2.5792}\n",
            "\n",
            "[131] train_loss=5.4155  test_loss=6.4935\n",
            "{'29': 7.9654, '30': 5.9746, '31': 7.0184, '32': 10.4794, '33': 8.7031, '34': 6.7528, '35': 7.2149, '36': 7.4185, '37': 6.6522, '38': 6.2663, '39': 5.1477, '40': 3.585, '41': 2.2097}\n",
            "\n",
            "[132] train_loss=5.2448  test_loss=4.8725\n",
            "{'29': 7.1273, '30': 4.1903, '31': 7.0287, '32': 6.7953, '33': 5.0669, '34': 5.2897, '35': 4.4512, '36': 5.2548, '37': 5.8951, '38': 4.651, '39': 3.9464, '40': 3.3495, '41': 3.061}\n",
            "\n",
            "[133] train_loss=4.7431  test_loss=5.5548\n",
            "{'29': 8.4143, '30': 6.3254, '31': 8.7124, '32': 7.4614, '33': 6.1088, '34': 6.5101, '35': 5.2617, '36': 5.8206, '37': 6.4744, '38': 4.9495, '39': 4.4075, '40': 3.758, '41': 3.3122}\n",
            "\n",
            "[134] train_loss=4.6630  test_loss=6.9477\n",
            "{'29': 9.2685, '30': 6.9557, '31': 9.7803, '32': 8.4204, '33': 6.5511, '34': 7.2391, '35': 6.5809, '36': 7.4016, '37': 8.4054, '38': 6.6766, '39': 6.2574, '40': 4.9876, '41': 4.0936}\n",
            "\n",
            "[135] train_loss=5.6766  test_loss=6.2792\n",
            "{'29': 8.6013, '30': 5.9302, '31': 8.4874, '32': 7.7494, '33': 5.7112, '34': 6.1803, '35': 5.481, '36': 6.4029, '37': 7.4817, '38': 6.1401, '39': 6.0947, '40': 5.2652, '41': 4.274}\n",
            "\n",
            "[136] train_loss=5.0512  test_loss=4.9692\n",
            "{'29': 6.5556, '30': 3.7787, '31': 5.5198, '32': 8.2182, '33': 6.4484, '34': 5.1936, '35': 5.1198, '36': 5.2465, '37': 5.2526, '38': 4.7888, '39': 4.1248, '40': 3.3072, '41': 2.39}\n",
            "\n",
            "[137] train_loss=4.9247  test_loss=5.5021\n",
            "{'29': 7.5459, '30': 4.8218, '31': 7.5341, '32': 7.0101, '33': 5.2564, '34': 5.6154, '35': 5.0157, '36': 5.737, '37': 6.7082, '38': 5.4241, '39': 4.9088, '40': 4.1416, '41': 3.5256}\n",
            "\n",
            "[138] train_loss=4.9149  test_loss=5.9099\n",
            "{'29': 8.2591, '30': 5.2178, '31': 7.9495, '32': 7.1842, '33': 5.5301, '34': 6.1941, '35': 5.5242, '36': 6.3784, '37': 7.3247, '38': 5.9184, '39': 4.999, '40': 4.1915, '41': 3.6999}\n",
            "\n",
            "[139] train_loss=6.4031  test_loss=5.4610\n",
            "{'29': 8.469, '30': 6.3107, '31': 6.9325, '32': 10.0488, '33': 7.8249, '34': 5.7438, '35': 5.7279, '36': 5.8336, '37': 5.3883, '38': 4.8998, '39': 4.3195, '40': 3.01, '41': 2.357}\n",
            "\n",
            "[140] train_loss=4.8760  test_loss=5.5571\n",
            "{'29': 8.5468, '30': 5.9491, '31': 8.1632, '32': 7.4116, '33': 5.5236, '34': 6.1032, '35': 5.2442, '36': 5.8422, '37': 6.6789, '38': 5.166, '39': 4.6563, '40': 3.8671, '41': 3.3795}\n",
            "\n",
            "[141] train_loss=4.7174  test_loss=4.5197\n",
            "{'29': 6.5685, '30': 2.9348, '31': 5.789, '32': 7.7375, '33': 5.447, '34': 4.5159, '35': 4.4294, '36': 4.8188, '37': 4.9904, '38': 4.2537, '39': 3.8624, '40': 2.9983, '41': 2.5711}\n",
            "\n",
            "[142] train_loss=4.6859  test_loss=4.9826\n",
            "{'29': 8.7269, '30': 5.9279, '31': 7.7437, '32': 7.1986, '33': 5.3814, '34': 5.5646, '35': 4.5933, '36': 5.0957, '37': 5.6889, '38': 4.5564, '39': 4.1031, '40': 3.441, '41': 3.1121}\n",
            "\n",
            "[143] train_loss=4.7822  test_loss=4.9662\n",
            "{'29': 9.8637, '30': 5.994, '31': 7.9754, '32': 7.5808, '33': 5.3068, '34': 5.6783, '35': 4.5993, '36': 5.3152, '37': 5.649, '38': 4.3975, '39': 3.9318, '40': 3.1573, '41': 2.9818}\n",
            "\n",
            "[144] train_loss=4.6690  test_loss=6.1856\n",
            "{'29': 7.1609, '30': 4.891, '31': 6.6013, '32': 9.8726, '33': 8.1251, '34': 6.3374, '35': 6.8379, '36': 7.1577, '37': 6.387, '38': 6.0387, '39': 4.9906, '40': 3.4826, '41': 2.2356}\n",
            "\n",
            "[145] train_loss=4.9528  test_loss=4.5226\n",
            "{'29': 7.3115, '30': 3.4165, '31': 5.6854, '32': 7.423, '33': 5.0215, '34': 4.5134, '35': 4.3419, '36': 4.7932, '37': 5.3645, '38': 4.1582, '39': 3.7278, '40': 3.2102, '41': 2.9641}\n",
            "\n",
            "[146] train_loss=4.9321  test_loss=4.8107\n",
            "{'29': 7.0586, '30': 3.514, '31': 6.5345, '32': 6.8067, '33': 5.0573, '34': 5.1243, '35': 4.1803, '36': 5.2057, '37': 5.7328, '38': 4.5431, '39': 4.0962, '40': 3.6727, '41': 3.1858}\n",
            "\n",
            "[147] train_loss=4.6338  test_loss=4.4893\n",
            "{'29': 6.7848, '30': 3.2861, '31': 5.9591, '32': 7.084, '33': 4.8419, '34': 4.4481, '35': 4.2299, '36': 4.6992, '37': 5.0105, '38': 4.4508, '39': 3.9639, '40': 3.2987, '41': 2.406}\n",
            "\n",
            "[148] train_loss=5.1025  test_loss=4.7152\n",
            "{'29': 6.5075, '30': 3.3312, '31': 5.919, '32': 7.0612, '33': 5.536, '34': 4.7881, '35': 4.2644, '36': 4.7689, '37': 5.5565, '38': 4.5286, '39': 4.0695, '40': 3.5524, '41': 3.1607}\n",
            "\n",
            "[149] train_loss=4.9300  test_loss=4.5112\n",
            "{'29': 6.4186, '30': 3.1024, '31': 5.5733, '32': 7.3198, '33': 5.441, '34': 4.2108, '35': 4.2434, '36': 4.8802, '37': 4.8798, '38': 4.7837, '39': 3.9664, '40': 2.7747, '41': 2.3639}\n",
            "\n",
            "[150] train_loss=4.9083  test_loss=5.1856\n",
            "{'29': 9.4959, '30': 6.6387, '31': 8.7239, '32': 7.5194, '33': 5.6536, '34': 5.931, '35': 4.7968, '36': 5.5251, '37': 6.0633, '38': 4.4427, '39': 3.9012, '40': 3.5783, '41': 3.0365}\n",
            "\n",
            "[151] train_loss=4.6272  test_loss=4.6490\n",
            "{'29': 6.1821, '30': 2.7316, '31': 5.6667, '32': 7.708, '33': 5.6235, '34': 4.6284, '35': 4.5571, '36': 4.9658, '37': 5.0658, '38': 4.5979, '39': 4.0811, '40': 3.0351, '41': 2.3304}\n",
            "\n",
            "[152] train_loss=4.7836  test_loss=4.5731\n",
            "{'29': 6.5238, '30': 2.8399, '31': 5.5683, '32': 7.6593, '33': 5.708, '34': 4.4436, '35': 4.5467, '36': 4.9611, '37': 4.9009, '38': 4.5702, '39': 3.9421, '40': 2.8535, '41': 2.2644}\n",
            "\n",
            "[153] train_loss=4.7298  test_loss=5.7346\n",
            "{'29': 9.4718, '30': 6.5968, '31': 8.5858, '32': 7.3773, '33': 5.7604, '34': 6.3221, '35': 5.3061, '36': 6.0021, '37': 6.7878, '38': 5.2682, '39': 4.8967, '40': 4.1334, '41': 3.4151}\n",
            "\n",
            "[154] train_loss=4.9127  test_loss=7.2738\n",
            "{'29': 13.1453, '30': 13.2299, '31': 14.3952, '32': 11.3197, '33': 9.6224, '34': 9.7963, '35': 7.5886, '36': 7.3868, '37': 7.6692, '38': 5.3437, '39': 4.6574, '40': 3.9706, '41': 3.4722}\n",
            "\n",
            "[155] train_loss=6.2957  test_loss=5.5564\n",
            "{'29': 5.8303, '30': 2.6228, '31': 5.4404, '32': 7.9207, '33': 6.2608, '34': 5.121, '35': 5.8561, '36': 6.6688, '37': 6.1574, '38': 5.9599, '39': 4.8635, '40': 3.3944, '41': 2.1904}\n",
            "\n",
            "[156] train_loss=5.4975  test_loss=5.0607\n",
            "{'29': 6.651, '30': 4.0828, '31': 6.8399, '32': 6.922, '33': 5.0402, '34': 5.1269, '35': 4.5911, '36': 5.3062, '37': 6.2714, '38': 4.8197, '39': 4.4203, '40': 3.8284, '41': 3.3938}\n",
            "\n",
            "[157] train_loss=4.8982  test_loss=4.4533\n",
            "{'29': 6.5624, '30': 3.0937, '31': 5.9002, '32': 7.0033, '33': 4.9448, '34': 4.3397, '35': 4.2091, '36': 4.7283, '37': 4.9098, '38': 4.5588, '39': 3.9575, '40': 2.9721, '41': 2.5324}\n",
            "\n",
            "[158] train_loss=5.7133  test_loss=4.4221\n",
            "{'29': 7.8807, '30': 3.8997, '31': 6.4193, '32': 6.8269, '33': 4.9264, '34': 4.6086, '35': 4.1073, '36': 4.6011, '37': 5.1324, '38': 4.173, '39': 3.6819, '40': 2.9189, '41': 2.9052}\n",
            "\n",
            "[159] train_loss=5.4899  test_loss=5.0676\n",
            "{'29': 9.527, '30': 7.2766, '31': 8.5271, '32': 7.3024, '33': 5.9043, '34': 5.7711, '35': 4.7295, '36': 4.9694, '37': 5.6746, '38': 4.2866, '39': 4.1276, '40': 3.5784, '41': 3.1906}\n",
            "\n",
            "[160] train_loss=4.9702  test_loss=4.4559\n",
            "{'29': 7.2672, '30': 3.9935, '31': 6.3801, '32': 7.0058, '33': 5.1328, '34': 4.519, '35': 4.2054, '36': 4.664, '37': 4.8562, '38': 4.3298, '39': 3.8291, '40': 3.0407, '41': 2.507}\n",
            "\n",
            "[161] train_loss=5.1479  test_loss=4.4573\n",
            "{'29': 7.1232, '30': 3.8209, '31': 6.3228, '32': 6.9862, '33': 4.9293, '34': 4.7295, '35': 4.2673, '36': 4.6712, '37': 5.1948, '38': 4.168, '39': 3.5497, '40': 3.1002, '41': 2.7265}\n",
            "\n",
            "[162] train_loss=4.9758  test_loss=5.2088\n",
            "{'29': 7.0408, '30': 2.9817, '31': 6.0586, '32': 7.4885, '33': 4.8199, '34': 4.5628, '35': 4.5158, '36': 5.2524, '37': 6.3454, '38': 5.4003, '39': 5.1509, '40': 4.3588, '41': 3.6473}\n",
            "\n",
            "[163] train_loss=4.8964  test_loss=4.6464\n",
            "{'29': 7.8469, '30': 3.5774, '31': 5.7579, '32': 8.0278, '33': 5.5747, '34': 4.7781, '35': 4.6285, '36': 4.8494, '37': 5.0515, '38': 4.4561, '39': 3.8942, '40': 2.8983, '41': 2.7519}\n",
            "\n",
            "[164] train_loss=4.8587  test_loss=6.6064\n",
            "{'29': 8.5749, '30': 6.6035, '31': 9.2153, '32': 8.2193, '33': 6.3205, '34': 6.8842, '35': 6.1665, '36': 6.9326, '37': 8.0013, '38': 6.1867, '39': 6.0301, '40': 4.9036, '41': 4.2619}\n",
            "\n",
            "[165] train_loss=4.9351  test_loss=4.4223\n",
            "{'29': 6.4303, '30': 2.7751, '31': 5.9672, '32': 7.4901, '33': 5.1167, '34': 4.4912, '35': 4.4339, '36': 4.6499, '37': 4.9345, '38': 4.1152, '39': 3.7177, '40': 2.9469, '41': 2.9198}\n",
            "\n",
            "[166] train_loss=6.5284  test_loss=9.1722\n",
            "{'29': 11.0392, '30': 9.3626, '31': 11.7971, '32': 9.8274, '33': 8.5316, '34': 9.6376, '35': 8.8877, '36': 9.1842, '37': 10.6736, '38': 9.1231, '39': 8.7678, '40': 7.555, '41': 5.4257}\n",
            "\n",
            "[167] train_loss=5.4164  test_loss=4.5992\n",
            "{'29': 7.5097, '30': 3.6461, '31': 6.353, '32': 7.2883, '33': 4.7881, '34': 4.3637, '35': 4.38, '36': 4.5931, '37': 4.9755, '38': 5.0647, '39': 4.1386, '40': 3.0568, '41': 2.3782}\n",
            "\n",
            "[168] train_loss=4.8039  test_loss=4.6170\n",
            "{'29': 10.0678, '30': 6.3048, '31': 7.7809, '32': 7.2674, '33': 5.0895, '34': 5.1466, '35': 4.0867, '36': 4.7419, '37': 4.9639, '38': 4.2783, '39': 3.7384, '40': 3.033, '41': 2.7023}\n",
            "\n",
            "[169] train_loss=4.4573  test_loss=4.8301\n",
            "{'29': 6.3262, '30': 2.7074, '31': 5.6834, '32': 7.5579, '33': 5.6233, '34': 4.6332, '35': 4.5554, '36': 5.141, '37': 5.1029, '38': 5.0948, '39': 4.4754, '40': 3.4743, '41': 2.2076}\n",
            "\n",
            "[170] train_loss=5.2686  test_loss=5.1587\n",
            "{'29': 6.7141, '30': 4.2748, '31': 5.9247, '32': 8.7262, '33': 6.817, '34': 5.2931, '35': 5.2563, '36': 5.6958, '37': 5.3322, '38': 4.985, '39': 4.2441, '40': 3.1451, '41': 2.3869}\n",
            "\n",
            "[171] train_loss=4.5059  test_loss=5.5062\n",
            "{'29': 6.3537, '30': 3.8164, '31': 5.9187, '32': 8.4232, '33': 6.799, '34': 5.3342, '35': 5.7711, '36': 6.2157, '37': 5.9179, '38': 5.7224, '39': 4.6813, '40': 3.2898, '41': 2.1518}\n",
            "\n",
            "[172] train_loss=5.3419  test_loss=4.4026\n",
            "{'29': 6.5507, '30': 3.1354, '31': 6.2424, '32': 7.2702, '33': 4.8486, '34': 4.4923, '35': 4.3997, '36': 4.563, '37': 4.9454, '38': 4.0976, '39': 3.8037, '40': 2.9607, '41': 2.8387}\n",
            "\n",
            "[173] train_loss=4.9760  test_loss=6.6565\n",
            "{'29': 10.4233, '30': 8.7708, '31': 9.9885, '32': 8.3493, '33': 6.9725, '34': 7.6995, '35': 6.7683, '36': 7.038, '37': 7.9034, '38': 6.1021, '39': 5.077, '40': 4.2584, '41': 3.5799}\n",
            "\n",
            "[174] train_loss=4.8756  test_loss=5.5907\n",
            "{'29': 10.0583, '30': 7.6306, '31': 9.1964, '32': 7.9792, '33': 6.0444, '34': 6.4617, '35': 5.4025, '36': 5.8077, '37': 6.394, '38': 5.0656, '39': 4.3005, '40': 3.4231, '41': 3.2206}\n",
            "\n",
            "[175] train_loss=5.3667  test_loss=4.9752\n",
            "{'29': 11.262, '30': 7.938, '31': 9.3466, '32': 8.2317, '33': 5.4772, '34': 5.6861, '35': 4.6994, '36': 5.1902, '37': 5.3736, '38': 4.3106, '39': 3.6686, '40': 2.926, '41': 2.7398}\n",
            "\n",
            "[176] train_loss=4.7847  test_loss=4.7200\n",
            "{'29': 6.5975, '30': 3.0531, '31': 6.1297, '32': 6.8893, '33': 5.2194, '34': 4.9627, '35': 4.6597, '36': 5.0273, '37': 5.7418, '38': 4.3604, '39': 3.7918, '40': 3.3294, '41': 3.1431}\n",
            "\n",
            "[177] train_loss=5.6685  test_loss=6.8119\n",
            "{'29': 7.7679, '30': 7.035, '31': 7.8844, '32': 11.4215, '33': 9.2717, '34': 7.327, '35': 7.7417, '36': 7.9165, '37': 7.0232, '38': 6.3599, '39': 5.0113, '40': 3.4132, '41': 2.222}\n",
            "\n",
            "[178] train_loss=5.6760  test_loss=4.5044\n",
            "{'29': 6.6352, '30': 2.8827, '31': 6.1963, '32': 7.1017, '33': 4.6673, '34': 4.4392, '35': 4.0301, '36': 4.7754, '37': 5.2993, '38': 4.4361, '39': 3.777, '40': 3.4155, '41': 3.1238}\n",
            "\n",
            "[179] train_loss=4.7327  test_loss=5.6617\n",
            "{'29': 7.831, '30': 4.5733, '31': 7.1691, '32': 6.9178, '33': 5.4622, '34': 5.7734, '35': 5.0756, '36': 6.0902, '37': 7.1122, '38': 5.4862, '39': 4.9278, '40': 4.397, '41': 3.9292}\n",
            "\n",
            "[180] train_loss=5.3687  test_loss=4.4711\n",
            "{'29': 6.9668, '30': 3.3036, '31': 5.9458, '32': 6.8996, '33': 4.9626, '34': 4.2325, '35': 4.232, '36': 4.6729, '37': 4.9696, '38': 4.615, '39': 4.0685, '40': 2.9254, '41': 2.515}\n",
            "\n",
            "[181] train_loss=4.3687  test_loss=4.4735\n",
            "{'29': 8.5149, '30': 3.6029, '31': 6.6498, '32': 7.0167, '33': 4.7417, '34': 4.457, '35': 4.1283, '36': 4.6469, '37': 4.9738, '38': 4.4894, '39': 3.8706, '40': 3.0811, '41': 2.5791}\n",
            "\n",
            "[182] train_loss=6.0854  test_loss=4.6171\n",
            "{'29': 6.8585, '30': 3.3427, '31': 5.6316, '32': 7.3544, '33': 5.6024, '34': 4.4839, '35': 4.495, '36': 4.8716, '37': 5.1116, '38': 4.5632, '39': 4.0065, '40': 3.1041, '41': 2.496}\n",
            "\n",
            "[183] train_loss=4.8652  test_loss=4.3788\n",
            "{'29': 6.7334, '30': 2.9721, '31': 5.8, '32': 7.0637, '33': 5.0858, '34': 4.3913, '35': 4.2789, '36': 4.5421, '37': 4.9852, '38': 4.1901, '39': 3.7322, '40': 2.9687, '41': 2.6482}\n",
            "\n",
            "[184] train_loss=4.8996  test_loss=4.9803\n",
            "{'29': 10.9824, '30': 6.7188, '31': 8.3663, '32': 8.2897, '33': 4.9868, '34': 4.7879, '35': 4.8582, '36': 4.813, '37': 5.3523, '38': 4.9173, '39': 4.4681, '40': 3.044, '41': 2.5339}\n",
            "\n",
            "[185] train_loss=4.3443  test_loss=4.6202\n",
            "{'29': 7.5083, '30': 3.8493, '31': 6.8859, '32': 7.1468, '33': 5.046, '34': 5.0131, '35': 4.4872, '36': 4.8759, '37': 5.2879, '38': 4.3473, '39': 3.8033, '40': 2.9387, '41': 2.7909}\n",
            "\n",
            "[186] train_loss=4.5556  test_loss=4.5457\n",
            "{'29': 7.5751, '30': 3.6122, '31': 6.4934, '32': 7.0625, '33': 4.8706, '34': 4.9358, '35': 4.2557, '36': 4.9343, '37': 5.1955, '38': 4.1979, '39': 3.7811, '40': 3.1198, '41': 2.7822}\n",
            "\n",
            "[187] train_loss=5.2025  test_loss=6.0414\n",
            "{'29': 7.6312, '30': 6.2354, '31': 6.9205, '32': 10.1817, '33': 8.0349, '34': 6.1777, '35': 6.6807, '36': 6.8414, '37': 6.2852, '38': 5.805, '39': 4.6411, '40': 3.1729, '41': 2.2679}\n",
            "\n",
            "[188] train_loss=5.1393  test_loss=4.5083\n",
            "{'29': 6.9999, '30': 3.5608, '31': 5.6093, '32': 7.8924, '33': 5.4599, '34': 4.49, '35': 4.3576, '36': 4.7239, '37': 4.9354, '38': 4.1935, '39': 3.9332, '40': 3.0028, '41': 2.525}\n",
            "\n",
            "[189] train_loss=4.4677  test_loss=4.5028\n",
            "{'29': 6.3337, '30': 2.9602, '31': 5.581, '32': 7.56, '33': 5.357, '34': 4.3666, '35': 4.2113, '36': 4.7985, '37': 4.8939, '38': 4.5159, '39': 3.9801, '40': 3.0318, '41': 2.446}\n",
            "\n",
            "[190] train_loss=4.6951  test_loss=5.0297\n",
            "{'29': 7.7114, '30': 4.3827, '31': 7.2065, '32': 7.0442, '33': 5.5498, '34': 5.6949, '35': 4.8012, '36': 5.4399, '37': 5.9878, '38': 4.6131, '39': 3.9852, '40': 3.2849, '41': 3.0237}\n",
            "\n",
            "[191] train_loss=4.5263  test_loss=6.4368\n",
            "{'29': 11.1619, '30': 9.2066, '31': 10.9832, '32': 9.2659, '33': 7.3996, '34': 7.7985, '35': 6.5109, '36': 6.8226, '37': 7.2677, '38': 5.449, '39': 4.8154, '40': 3.612, '41': 3.2133}\n",
            "\n",
            "[192] train_loss=4.6158  test_loss=4.4019\n",
            "{'29': 6.729, '30': 2.9244, '31': 5.9289, '32': 7.0696, '33': 4.8138, '34': 4.4389, '35': 4.1824, '36': 4.694, '37': 4.928, '38': 4.2532, '39': 3.8753, '40': 3.0695, '41': 2.4214}\n",
            "\n",
            "[193] train_loss=4.6889  test_loss=4.3894\n",
            "{'29': 7.0986, '30': 2.9268, '31': 5.9348, '32': 7.2607, '33': 4.9978, '34': 4.3772, '35': 4.3563, '36': 4.6026, '37': 5.0869, '38': 4.076, '39': 3.7294, '40': 2.8781, '41': 2.6558}\n",
            "\n",
            "[194] train_loss=5.4985  test_loss=4.5560\n",
            "{'29': 7.0517, '30': 2.9913, '31': 6.2715, '32': 6.8903, '33': 4.9995, '34': 4.8607, '35': 4.1972, '36': 4.8181, '37': 5.3955, '38': 4.2855, '39': 3.8077, '40': 3.2109, '41': 2.9905}\n",
            "\n",
            "[195] train_loss=5.1167  test_loss=4.3913\n",
            "{'29': 6.9095, '30': 3.1074, '31': 5.5888, '32': 7.5878, '33': 5.169, '34': 4.3928, '35': 4.035, '36': 4.5652, '37': 5.0033, '38': 4.1018, '39': 3.6351, '40': 3.2871, '41': 2.8104}\n",
            "\n",
            "[196] train_loss=5.5264  test_loss=5.2291\n",
            "{'29': 6.6725, '30': 3.5162, '31': 5.3309, '32': 7.6383, '33': 5.8817, '34': 4.8579, '35': 5.3273, '36': 5.9675, '37': 5.8002, '38': 5.5685, '39': 4.6017, '40': 3.3179, '41': 2.23}\n",
            "\n",
            "[197] train_loss=4.2160  test_loss=4.5918\n",
            "{'29': 6.1423, '30': 2.4718, '31': 5.516, '32': 7.4914, '33': 5.3407, '34': 4.354, '35': 4.3908, '36': 4.7493, '37': 4.8782, '38': 4.7235, '39': 4.3083, '40': 3.3359, '41': 2.28}\n",
            "\n",
            "[198] train_loss=4.9156  test_loss=5.0821\n",
            "{'29': 8.4148, '30': 5.0015, '31': 7.4454, '32': 6.9948, '33': 5.5835, '34': 5.7521, '35': 4.8056, '36': 5.5359, '37': 6.078, '38': 4.5566, '39': 3.9746, '40': 3.4045, '41': 3.0085}\n",
            "\n",
            "[199] train_loss=4.8733  test_loss=5.9064\n",
            "{'29': 10.2817, '30': 7.495, '31': 9.4633, '32': 8.0145, '33': 6.4348, '34': 6.9252, '35': 5.8004, '36': 6.292, '37': 6.8237, '38': 4.978, '39': 4.7331, '40': 3.859, '41': 3.3163}\n",
            "\n",
            "[200] train_loss=5.1016  test_loss=5.8045\n",
            "{'29': 10.0411, '30': 6.2739, '31': 8.5145, '32': 7.7809, '33': 5.5183, '34': 6.1312, '35': 5.5423, '36': 6.3239, '37': 6.9777, '38': 5.4303, '39': 4.9027, '40': 3.9248, '41': 3.3394}\n",
            "\n"
          ]
        }
      ],
      "execution_count": 240
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report Generation"
      ],
      "metadata": {
        "id": "Da-4OnM6Kz9S"
      },
      "id": "Da-4OnM6Kz9S"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(\n",
        "\n",
        "    original_count    : int,\n",
        "    cleaned_count     : int,\n",
        "    train_count       : int,\n",
        "    test_count        : int,\n",
        "    gest_counts       : dict[int, int],\n",
        "    train_losses      : list[float],\n",
        "    test_losses       : list[float],\n",
        "    group_losses      : list[dict[str, float]],\n",
        "    group_losses_week : dict[str, float],\n",
        "    model_name        : str,\n",
        "    n_epochs          : int | None,\n",
        "    output_path       : str\n",
        "\n",
        "):\n",
        "\n",
        "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with PdfPages(output_path) as pdf:\n",
        "\n",
        "        # --- Page 1: Summary ---\n",
        "        plt.figure(figsize=(8,6)) ; plt.axis('off')\n",
        "        summary_text = (\n",
        "            f\"Model: {model_name}\\n\"\n",
        "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n",
        "            f\"Measurements (original): {original_count}\\n\"\n",
        "            f\"Measurements (cleaned) : {cleaned_count}\\n\"\n",
        "            f\"Train/Test: {train_count}/{test_count}\\n\\n\"\n",
        "            f\"Epochs: {n_epochs}\\n\\n\"\n",
        "            f\"Gestational Age Counts:\\n\" +\n",
        "            \"\\n\".join([f\"{int(k)} weeks: {int(v)}\" for k, v in sorted(gest_counts.items())])\n",
        "        )\n",
        "        plt.text(0, 1, summary_text, va='top', fontsize=10, family='monospace')\n",
        "        pdf.savefig() ; plt.close()\n",
        "\n",
        "        # --- Page 2: Overall Loss Plot ---\n",
        "        plt.figure(figsize=(10,8))\n",
        "        x = np.arange(1, len(train_losses) + 1)\n",
        "        plt.plot(x, train_losses, label='Train MAE')\n",
        "        plt.plot(x, test_losses,  label='Test MAE')\n",
        "        plt.xlabel('Epoch'); plt.ylabel('MAE'); plt.title('Train vs Test MAE')\n",
        "        plt.legend() ; plt.grid(True) ; plt.tight_layout()\n",
        "        pdf.savefig() ; plt.close()\n",
        "\n",
        "        # --- Page 3: Per-Gestational-Week Plot (robust to pd.NA/missing) ---\n",
        "        df_group = pd.DataFrame(group_losses_week)\n",
        "        df_group = df_group.replace({pd.NA: np.nan}).apply(pd.to_numeric, errors='coerce')\n",
        "        def _to_week(c):\n",
        "            try: return int(float(c))\n",
        "            except Exception: return c\n",
        "        df_group = df_group.rename(columns=_to_week)\n",
        "        plt.figure(figsize=(10,8))\n",
        "        x = np.arange(1, len(df_group) + 1)\n",
        "        plotted = 0\n",
        "        for col in sorted(df_group.columns, key=lambda k: (isinstance(k, int), k)):\n",
        "            y = df_group[col].to_numpy(dtype=float)\n",
        "            if np.isfinite(y).any():\n",
        "                label = f'{col}w' if isinstance(col, int) else str(col)\n",
        "                plt.plot(x, y, label=label)\n",
        "                plotted += 1\n",
        "        plt.xlabel('Epoch') ; plt.ylabel('Test MAE') ; plt.title('Test MAE by Gestational Age (weeks)')\n",
        "        if plotted: plt.legend(ncol=3)\n",
        "        plt.grid(True) ; plt.tight_layout()\n",
        "        pdf.savefig() ; plt.close()\n",
        "\n",
        "        # --- Page 4: Per-epoch Table ---\n",
        "        def _format_group_line(d):\n",
        "            parts = [\n",
        "                f\"{int(float(k))}w={float(v):.3f}\"\n",
        "                for k, v in sorted(d.items(), key=lambda kv: float(kv[0]))\n",
        "                if pd.notna(v)\n",
        "            ]\n",
        "            return \", \".join(parts)\n",
        "\n",
        "        lines = [\n",
        "            f\"[{i+1:03d}]  train={t:.4f}  test={v:.4f}\\n{_format_group_line(g)}\\n\"\n",
        "            for i, (t, v, g) in enumerate(zip(train_losses, test_losses, group_losses))\n",
        "        ]\n",
        "\n",
        "        visual_rows = sum(s.count(\"\\n\")+2 for s in lines)\n",
        "\n",
        "        fontsize = 10\n",
        "        line_h_in = fontsize * 1.35 / 72.0\n",
        "        top_margin_in = 0.6\n",
        "        bottom_margin_in = 0.6\n",
        "        fig_w_in = 8.5\n",
        "        fig_h_in = top_margin_in + bottom_margin_in + line_h_in * visual_rows\n",
        "        plt.figure(figsize=(fig_w_in, fig_h_in))\n",
        "        plt.axis('off')\n",
        "        ax = plt.gca()\n",
        "        ax.text(\n",
        "            0, 1,\n",
        "            \"\\n\".join(lines),\n",
        "            va='top', ha='left',\n",
        "            transform=ax.transAxes,\n",
        "            family='monospace',\n",
        "            fontsize=fontsize,\n",
        "            wrap=True,\n",
        "        )\n",
        "\n",
        "        pdf.savefig(); plt.close()\n",
        "\n",
        "\n",
        "    print(f\"Report saved to {output_path}\")"
      ],
      "metadata": {
        "id": "xhfAPrF52ti6"
      },
      "id": "xhfAPrF52ti6",
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = f'ts2vec-03-static'\n",
        "\n",
        "generate_report(\n",
        "    original_count=len(data),\n",
        "    cleaned_count=len(cleaned_data),\n",
        "    train_count=len(train),\n",
        "    test_count=len(test),\n",
        "    gest_counts=cleaned_df[\"gest_age_weeks\"].value_counts().sort_index().to_dict(),\n",
        "    train_losses=train_losses,\n",
        "    test_losses=test_losses,\n",
        "    group_losses=group_losses,\n",
        "    group_losses_week=group_losses_week,\n",
        "    model_name=f'{model_name}-{target_var}',\n",
        "    n_epochs=n_epochs,\n",
        "    output_path=f'/content/drive/MyDrive/reports/{model_name}/{model_name}-{target_var}-report.pdf'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzcu1WmeLd-E",
        "outputId": "0fccfc39-f3f7-4312-f43c-d645fbceb22b"
      },
      "id": "yzcu1WmeLd-E",
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2186282221.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_group = df_group.replace({pd.NA: np.nan}).apply(pd.to_numeric, errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report saved to /content/drive/MyDrive/reports/ts2vec-03-static/ts2vec-03-static-onset-report.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KQCu0syptDj"
      },
      "id": "_KQCu0syptDj",
      "execution_count": 242,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59177ad186a241319682a0521ddde247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18ad063a55504a8ea7cc790828b1bb80",
              "IPY_MODEL_8cf9b36a7e0746f9b013458e269926d7",
              "IPY_MODEL_01362b0c571243ba8f0ef684072ebdc1"
            ],
            "layout": "IPY_MODEL_5fdbb09a919b4309b6641676fd1df424"
          }
        },
        "18ad063a55504a8ea7cc790828b1bb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2b81e05ce034576857be1892346d8f0",
            "placeholder": "",
            "style": "IPY_MODEL_241e58f74f924657924caf7202e2a1ac",
            "value": "100%"
          }
        },
        "8cf9b36a7e0746f9b013458e269926d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f93de2488b4ddaa7a6d4383c3c30af",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32c8128e0861409ab9d7767a1d874564",
            "value": 200
          }
        },
        "01362b0c571243ba8f0ef684072ebdc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bec2e9d387946d0871e4e2b324bf72c",
            "placeholder": "",
            "style": "IPY_MODEL_5367ba18fc4c49e8846b9f3c3dc7f59a",
            "value": "200/200[05:32&lt;00:00,1.66s/it]"
          }
        },
        "5fdbb09a919b4309b6641676fd1df424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b81e05ce034576857be1892346d8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241e58f74f924657924caf7202e2a1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7f93de2488b4ddaa7a6d4383c3c30af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c8128e0861409ab9d7767a1d874564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bec2e9d387946d0871e4e2b324bf72c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5367ba18fc4c49e8846b9f3c3dc7f59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}